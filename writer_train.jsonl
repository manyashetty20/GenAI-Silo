{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nBridge the Domains: Large Language Models Enhanced Cross-domain\n  Sequential Recommendation\n\n[NUGGETS]\n- Cross-domain Sequential Recommendation (CDSR) aims to extract the preference\nfrom the user's historical interactions across various domains [ref_1]\n- Despite some\nprogress in CDSR, two problems set the barrier for further advancements, i [ref_2]\n- ,\noverlap dilemma and transition complexity [ref_4]\n- The former means existing CDSR\nmethods severely rely on users who own interactions on all domains to learn\ncross-domain item relationships, compromising the practicability [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n", "output": "Cross-domain Sequential Recommendation . Recommender\nsystems [ 23,32,48,49,61,62] become important in life since\nthey can alleviate the problem of information overload. Recently,\nCDSR [ 7] has emerged, absorbing both merits from cross-domain\nrecommendation [ 10,14,18,19,22,51,58] and sequential recom-\nmendation [ 17,24,29,31]. The key to CDSR lies in filling the distri-\nbution gap between various domains. From the item aspect, most\ncurrent studies adopt Graph Neural Networks (GNN) [ 53] to estab-\nlish the item connections across domains. As one of the pioneers,\nC2DSR [ 5] proposes to build an interaction graph based on the\nco-occurrences of the items across two domains. Following C2DSR,\nEA-GCL [ 47] and MGCL [ 56] further design contrastive objectives\nfor graph learning to alleviate density bias and transition difficulty.\nFrom the user perspective, CDSR works often design more sophis-\nticated architectures to capture global preferences from the mixedbehavior sequences. \ud835\udf0b-Net [ 35], an early work in this line, pro-\nposes a cross-domain transfer module to derive hybrid preferences.\nRecently, TriCDR [ 34] has focused on modeling mixed sequences,\ndesigning triple contrastive tasks to dig into fine-grained global\ninterests. Though existing CDSR works have explored various meth-\nods to bridge the domains, they are trapped in a collaborative view.\nIn contrast, we propose a semantic view to enhance CDSR by LLMs.\nLLMs for Sequential Recommendation . LLMs have been ap-\nplied to several fields [ 27,54], including recommender systems [ 2,9,\n20,25,28,43,46,50,52]. Relying on powerful abilities in reasoning\nand understanding [ 59], LLMs can benefit SRS by analyzing users\u2019\nbehaviors and items\u2019 attributes. Existing LLMs for SRS works can\nbe categorized into two main lines, i.e.,LLMs as SRS and LLMs\nenhancing SRS. The first category refers to utilizing LLMs to gener-\nate recommendations directly. For example, GOT4Rec [ 33] designs\nchain-of-thoughts to prompt LLMs giving out recommendations.\nTo fill the gap between natural language and recommendation tasks,\nsome propose to fine-tune open-sourced LLMs, e.g.,TALLRec [ 4]\nand S-DPO [ 8]. The other line, i.e.,LLMs enhancing SRS, often\nadopts LLMs as item or user encoders [ 30]. Since the LLMs em-\nbeddings can be cached in advance, they are more practical due to\nno need for LLMs while serving. SAID [ 13] and TSLRec [ 21] are\ntwo representatives, which use the LLMs embeddings to initialize\nthe embedding layer of SRS. To better combine collaborative and\nsemantic information, LLM-ESR [ 26] designs a dual-view modeling\nframework. It is worth noting that we are the first to explore how\nto utilize LLMs to enhance CDSR."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nDual Debiasing for Noisy In-Context Learning for Text Generation\n\n[NUGGETS]\n- In context learning (ICL) relies heavily on high quality demonstrations drawn\nfrom large annotated corpora [ref_1]\n- Existing approaches detect noisy annotations by\nranking local perplexities, presuming that noisy samples yield higher\nperplexities than their clean counterparts [ref_2]\n- However, this assumption breaks\ndown when the noise ratio is high and many demonstrations are flawed [ref_3]\n- We\nreexamine the perplexity based paradigm for text generation under noisy\nannotations, highlighting two sources of bias in perplexity: the annotation\nitself and the domain specific knowledge inherent in large language models\n(LLMs) [ref_4]\n- To overcome these biases, we introduce a dual debiasing framework that\nuses synthesized neighbors to explicitly correct perplexity estimates, yielding\na robust Sample Cleanliness Score [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "In-context learning (ICL): Recent research has\nleveraged pre-trained LLMs for downstream NLP\ntasks through in-context learning, particularly in\ntext classification (Yoo et al., 2022) and genera-\ntion tasks (O\u2019Brien and Lewis, 2023). Notable\nadvances include the UDR retriever by Li et al.\n(2023), which works effectively across multiple\ntasks, and the efficient approach by Liu et al. that\nextracts in-context vectors from LLM embeddings\nto reduce computational costs. However, most ICL\nresearch assumes clean, high-quality demonstra-\ntions, leaving open questions about performance\nwith noisy or imperfect examples.\nICL with noisy annotations: Initial studies ex-\nploring random labels in ICL classification have\nshown mixed results. While Min et al. (2022) found\nlimited performance impact with random retrievers\nfor certain LLM-dataset combinations, Yoo et al.\n(2022) demonstrated significant performance degra-\ndation across a broader range of settings. More re-\ncent work has begun addressing noisy ICL directly.\nKang et al. (2024) proposed Rectification for clas-\nsification tasks, though its fine-tuning requirements\nintroduce substantial computational overhead. For\ngeneration tasks, Gao et al. (2024) pioneered the\nfirst noise-robust method, but it shows limitations\nunder high-noise conditions.\nDebiasing LLM Output: Despite their capa-\nbilities, LLMs can exhibit biases from their pre-\ntraining corpora that impact task performance. To\naddress this, Li et al. (2022) and Zhao et al. (2024)\ndeveloped Contrastive Decoding, which improves\ntext generation quality by debiasing larger LLMs\nusing outputs from smaller models within the same\nfamily. Additionally, Fei et al. (2023) and Zhao\net al. (2021) introduced methods to reduce bias in\nLLMs by addressing both prefixed context bias and\nfinite label bias in classification tasks."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nBenchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph\n\n[NUGGETS]\n- The rapid growth of open source machine learning (ML) resources, such as\nmodels and datasets, has accelerated IR research [ref_1]\n- However, existing platforms\nlike Hugging Face do not explicitly utilize structured representations,\nlimiting advanced queries and analyses such as tracing model evolution and\nrecommending relevant datasets [ref_2]\n- To fill the gap, we construct HuggingKG, the\nfirst large-scale knowledge graph built from the Hugging Face community for ML\nresource management [ref_3]\n- 6 million nodes and 6 [ref_5]\n\n[REFERENCES]\n[1] GitHub 49,200 4 9 95,160Issue ,PR,\nRepository ,UserNo Yes Issue-PR Link Prediction\nHuggingKG &HuggingBench Hugging Face 2,614,270 8 30 6,246,353Model ,Dataset ,\nUser,Task\n(Description)Yes\n(Finetune ,A\n", "output": "KGs for Resource Management. KGs have been extensively used\nto represent and analyze complex relationships in various domains,\nincluding open source resource management. Previous works such\nas DEKR [ 6] and MLTaskKG [ 23] have constructed KGs to support\nrecommendation tasks by capturing relationships among ML re-\nsources. DEKR [ 6] primarily relies on description enhancement for\nML method recommendation. MLTaskKG [ 23] constructs an AI task-\nmodel KG by integrating static data to support task-oriented ML/DL\nlibrary recommendation. However, both approaches focus on static\nattributes and a narrow set of relations, failing to capture dynamic\nuser interactions and inter- Model relations. In contrast, as shown in\nTable 1, our proposed HuggingKG is built on the rich metadata pro-\nvided by Hugging Face, offering a large-scale KG with a more exten-\nsive set of relations. In addition to generic relations (e.g., Defined\nFor,Cite ),HuggingKG incorporates multiple inter- Model relations\n(i.e., Adapter ,Finetune ,Merge , and Quantize ) and captures user\ninteraction signals (i.e., Publish ,Like , and Follow ).This enriched\nstructure facilitates a deeper analysis of ML resources and supports\nmore effective recommendation strategies.\nKG-based Benchmarks. Various benchmark datasets have been\nproposed to evaluate KG-based tasks. For example, OAG-Bench [ 51]\nprovides a human-curated benchmark for academic graph mining,\nfocusing on citation and collaboration networks. In the domain of\nopen source resource management, our HuggingBench benchmark\ndistinguishes itself by providing datasets for three IR tasks: resource\nrecommendation, task classification, and model tracing.\nForresource recommendation , paper2repo [ 32] introduces\na distant-supervised recommender system that matches papers\n1https://huggingface.co/collections/cqsss/huggingbench-67b2ee02ca45b15e351009a2\n2https://github.com/nju-websoft/HuggingBenchwith related code repositories. However, it incorporates a limited\nrange of entity types that are insufficient to build fine-grained in-\nterdependencies. Xu et al. [ 44] leverages multi-modal features from\ndevelopers\u2019 sequential behaviors and repository text to generate\nrelevant and tailored suggestions for developers, yet it does not ex-\nplicitly construct or exploit a structured KG. In contrast, as shown\nin Table 1, HuggingBench benefits from the inherent structure of\nHuggingKG that captures rich relational data for recommendation .\nFurthermore, GRETA [ 5] and recent efforts in automated cate-\ngorization [ 25,34] address specific tagging/classification tasks.\nGRETA [ 5] constructs an Entity Tag Graph (ETG) using the cross-\ncommunity knowledge from GitHub and Stack Overflow, and uses\nan iterative random walk with restart algorithm to automatically\nassign tags to repositories. HuggingKG integrates richer textual\ndescriptions and metadata to construct a graph that encapsulates fine-\ngrained relationships among models and datasets, thereby facilitating\nmulti-label task classification for ML resources.\nRecent work by Bai et al. [ 1] uses a knowledge-aware heteroge-\nneous graph learning approach to predict links between issues\nand pull requests on GitHub, effectively capturing complex rela-\ntional information through metapath aggregation. However, it re-\nmains confined to linking Issue \u2013PRpairs and does not address the\nbroader challenge of tracking model evolution across ML resources.\nThe novel model tracing task in HuggingBench not only pioneers\nthe exploration of inter- Model relations, but also provides practical\ninsights into the evolution, reuse, and optimization of ML models ,\nthereby supporting more informed decision-making in real-world\nopen source resource management.\n3HuggingKG Knowledge Graph\n3.1 KG Construction\nThe construction of HuggingKG follows a principled process that\nincludes defining nodes and edges, crawling and converting data\nfrom the Hugging Face community website, and performing data\nverification and cleaning.\nSchema Definit"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nNLCTables: A Dataset for Marrying Natural Language Conditions with Table\n  Discovery\n\n[NUGGETS]\n- With the growing abundance of repositories containing tabular data,\ndiscovering relevant tables for in-depth analysis remains a challenging task [ref_1]\n- Existing table discovery methods primarily retrieve desired tables based on a\nquery table or several vague keywords, leaving users to manually filter large\nresult sets [ref_2]\n- To address this limitation, we propose a new task: NL-conditional\ntable discovery (nlcTD), where users combine a query table with natural\nlanguage (NL) requirements to refine search results [ref_3]\n- To advance research in\nthis area, we present nlcTables, a comprehensive benchmark dataset comprising\n627 diverse queries spanning NL-only, union, join, and fuzzy conditions, 22,080\ncandidate tables, and 21,200 relevance annotations [ref_4]\n- Our evaluation of six\nstate-of-the-art table discovery methods on nlcTables reveals substantial\nperformance gaps, highlighting the need for advanced techniques to tackle this\nchallenging nlcTD scenario [ref_5]\n\n[REFERENCES]\n[2] and\nWebTable\n[11] are designed to progressively increase in\nscale, the other datasets remain relatively small and are derived\nfrom single data sources. This lack of diversity makes them less\n4\nTable 1: Characteristics \n[14] consider semantic overlap\n(e.g., \u201c Incorporation \u201d and \u201c Inc. \u201d) through representation learn-\ning. Typically, they first encode the columns, then index these\ncolumns, and finally search joinable pair\n[16] have adopted ta-\nble representation learning\n[19] focus on table union search scenarios, where query tables\nare provided as input and ground truth is generated via table split-\nting (see Section 4.2). LakeBench\n[24] and San-\ntos\n[31] serializes tables to a se-\nquence of tokens). While keyword-based approaches are highly\nvaluable, there remains significant room for improvement. nlcTD\ndefined in Section 2.1 aims to extend this parad\n[41] considers only exact value overlap using set\nsimilarity search. Here, \ud835\udc36\ud835\udc5eis treated as a set, and the top- \ud835\udc58columns\nwith the highest value overlap are returned. Recent embedding-\nbased approaches such \n", "output": "Section 3.1 goes through current table discovery methods, highlight-\ning the need and opportunities for developing nlcTD techniques.\nSection 3.2 examines existing test data collections, identifying gaps\nthat must be addressed to facilitate nlcTD .\n3.1 Table Discovery Approaches\nKeyword-based Table Search [31,33,34] retrieves a ranked list\nof table instances from the repository, ordered by their relevance\nscores in relation to one or more user-provided keywords. These\napproaches allow users to locate data with minimal prior knowl-\nedge of the structure or relationships within the table repository.\nHowever, they typically accept sorely keywords as input and treat\ntables as plain text (e.g., Strubert [31] serializes tables to a se-\nquence of tokens). While keyword-based approaches are highly\nvaluable, there remains significant room for improvement. nlcTD\ndefined in Section 2.1 aims to extend this paradigm by supporting\ninputs that go beyond simple keywords, allowing for more complex\nnatural language queries, such as full sentences. Nonetheless, tak-\ning only NL as input is only a special case in our nlcTD scenarios.\nIn practice, users often already have a table at hand and would use\nit as query context to refine their table retrieval results.\nQuery-table-based Table Search identifies additional tables rel-\nevant to a given query table, leveraging its header and/or body\ncontent. The literature has mainly seen two categories of work.\nTable Union Search [3,16,17,19,24] retrieves tables that are\nunion-compatible with the query table, meaning these two tables\nhaving multiple pairs of columns that can be merged. This involvesassessing similarity between query table and candidate tables. Early\napproach Tus[24] defines three probabilistic models to measure\nthe similarity between column values, column domains, and word\nembeddings of column content. Santos [19] utilizes knowledge\ngraph to measure the possibility that columns originate from the\nsame domain. Recently, methods like Starmie [16] have adopted ta-\nble representation learning [ 10,17] to generate column embeddings\nfor similarity measure, allowing a more nuanced understanding of\ntable contexts.\nTable Join Search [8,12\u201314,22,35,41] retrieves tables that are\njoinable with the query table at a specified column \ud835\udc36\ud835\udc5e, meaning the\ntwo tables sharing overlapping or semantically related values in\nthat column. Josie [41] considers only exact value overlap using set\nsimilarity search. Here, \ud835\udc36\ud835\udc5eis treated as a set, and the top- \ud835\udc58columns\nwith the highest value overlap are returned. Recent embedding-\nbased approaches such as DeepJoin [14] consider semantic overlap\n(e.g., \u201c Incorporation \u201d and \u201c Inc. \u201d) through representation learn-\ning. Typically, they first encode the columns, then index these\ncolumns, and finally search joinable pairs based on the index.\nQuery-table-based search is commonly used to identify table\ncomponents that can augment or complement an existing query ta-\nble. While nlcTD encompasses this functionality \u2014 allowing users\nto specify conditions like retrieving unionable or joinable tables\n\u2014nlcTD goes beyond these by supporting more flexible and cus-\ntomizable NL conditions. Not limited to predefined operations (e.g.,\nset similarity or semantic encoding), nlcTD allows users to specify\nnuanced requests in natural language. Existing approaches struggle\nwith nlcTD as they fail to capture and utilize the rich semantics of\nthese conditions (see our empirical study in Section 5.2).\n3.2 Existing Test Data Collections\nTo the best of our knowledge, no prior work has specifically con-\nstructed a dataset for NL-conditional table discovery, despite its\nsignificant application potential. Existing tabular data collections\nprimarily focus on table retrieval (or table discovery) in either\nkeyword-based or query-table-based paradigms. Table 1 summa-\nrizes representative datasets, highlighting their focus, dataset scales,\nand the availability of ground truth annotations for benchmarking.\nFor exam"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nGraph Spectral Filtering with Chebyshev Interpolation for Recommendation\n\n[NUGGETS]\n- Graph convolutional networks have recently gained prominence in collaborative\nfiltering (CF) for recommendations [ref_1]\n- However, we identify potential bottlenecks\nin two foundational components [ref_2]\n- First, the embedding layer leads to a latent\nspace with limited capacity, overlooking locally observed but potentially\nvaluable preference patterns [ref_3]\n- Also, the widely-used neighborhood aggregation is\nlimited in its ability to leverage diverse preference patterns in a\nfine-grained manner [ref_4]\n- Building on spectral graph theory, we reveal that these\nlimitations stem from graph filtering with a cut-off in the frequency spectrum\nand a restricted linear form [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Graph Neural Networks. Beside random-walk based approaches\n[14,23,25,43,47,64], there are two primary approaches in Graph\nNeural Networks (GNNs): spatial and spectral graph convolutions.\nSpatial graph convolution defines convolution in the vertex domain.\nGCN [ 29] simplifies graph spectral convolutions using only the\nfirst-order linear filters, equivalent to the spatial convolution of\n1-hop neighbor aggregation. SGC [ 66] reduces its computational\ncomplexity by removing its redundant feature transformations.\nMore recent works have expanded GCNs to a broader range of\napplications [13, 16, 59, 70].\nSpectral graph convolution relies on costly Laplacian eigende-\ncomposition to perform convolution in the spectral domain, trig-\ngering the development of numerous polynomial approximations\nto circumvent this issue. ChebNet [ 8], for example, adopts a Cheby-\nshev basis. GPR-GNN [ 4] and BernNet [ 18] utilize the monomial and\nBernstein basis, respectively. ChebNetII [ 3] presents an improved\nmodel using Chebyshev interpolation, which reduces the Runge\nphenomenon. Inspired by the strong performance of graph filtering\nusing polynomial approximations in the node classification task, we\ndesign ChebyCF\u2014a graph filter utilizing Chebyshev interpolation,\nspecifically adapted to the context of collaborative filtering, includ-\ning the elimination of heavy node-wise feature transformations.\nGraph-based Recommendations. Interpreting the user-item in-\nteraction as a graph, GNNs have been extensively explored in col-\nlaborative filtering. After GCNs are applied to CF for the first time\n[62], several following studies [ 3,21] have shown that the inher-\nent complexity of GCNs is less suitable for CF. This is because\nCF relies only on the user-item interaction data without any fea-\nture information. To address this, several attempts to simplify the\nmodel structure have been made [ 17,41]. There are also various\napproaches [ 10,15,27,28,30,38,55,63,65,73] to capture impor-\ntant information from the user-item interaction graph. Additionally,\nthere have been attempts [ 24,37,67,69] to overcome the sparsity\nof interaction data by leveraging graph contrastive learning.\nWhile many of the aforementioned studies have proposed GCN-\nbased models with spatial convolution, there are also GSF-based\napproaches utilizing spectral convolution [ 11,39,45,46,52,72].\nGraph Spectral Filtering with Chebyshev Interpolation for Recommendation SIGIR \u201925, July 13\u201318, 2025, Padova, Italy\nLinkProp [ 11] interprets CF from the perspective of the graph link\nprediction task. GF-CF [ 52] highlights that the power of existing\nspatial convolution approaches lies in their low-pass filtering capa-\nbilities from the spectral filtering perspective. PGSP [ 39] builds on\nGF-CF by augmenting both the input signal and graph. SGFCF [ 46]\nfurther enhances GF-CF through the introduction of a new graph\nnormalization technique and individualized filtering. Nevertheless,\nthe form of graph filters has largely remained linear, with limited\nattention given to enabling more flexible formulations. Our method\naddresses this by leveraging Chebyshev interpolation with a graph\nfilter that is both flexible and computationally efficient."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nNexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization\n\n[NUGGETS]\n- Summarizing long-form narratives--such as books, movies, and TV\nscripts--requires capturing intricate plotlines, character interactions, and\nthematic coherence, a task that remains challenging for existing LLMs [ref_1]\n- We\nintroduce NexusSum, a multi-agent LLM framework for narrative summarization\nthat processes long-form text through a structured, sequential\npipeline--without requiring fine-tuning [ref_2]\n- Our approach introduces two key\ninnovations: (1) Dialogue-to-Description Transformation: A narrative-specific\npreprocessing method that standardizes character dialogue and descriptive text\ninto a unified format, improving coherence [ref_3]\n- (2) Hierarchical Multi-LLM\nSummarization: A structured summarization pipeline that optimizes chunk\nprocessing and controls output length for accurate, high-quality summaries [ref_4]\n- Our\nmethod establishes a new state-of-the-art in narrative summarization, achieving\nup to a 30 [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Narrative summarization differs from traditional\ndocument summarization, requiring specialized\ntechniques to handle complex plots, evolving char-\nacters, and mixed prose-dialogue structures. This\nsection reviews related work on narrative summa-\nrization, long-context summarization, and multi-\nagent LLMs, positioning NEXUS SUMwithin this\nresearch landscape.\n2.1 Narrative Summarization\nBenchmark datasets like BookSum, MENSA,\nMovieSum and SummScreenFD have advanced\nlong-form narrative summarization research. Tra-\nditional extractive-to-abstractive pipelines (Ladhak\net al., 2020; Liu et al., 2022) risk losing coherence\nby omitting character arcs and event dependencies.\nTo address this, scene-based and discourse-aware\ntechniques leverage graph-based models (Gorinski\nand Lapata, 2015) and transformer-based saliency\nclassifiers (Saxena and Keller, 2024b). However,\nthese methods struggle with full text processing,\noften truncating key content. Our approach over-\ncomes this gap by introducing the dialogue-to-\ndescription transformation, allowing for a holistic\nnarrative processing while preserving coherence.2.2 Long-Context Summarization\nLong-context summarization techniques typically\nfall into two categories:\nArchitectural Optimization Transformer mod-\nels struggle with scalability due to the quadratic\ncost of self-attention. Solutions include sparse\nattention, memory-efficient encoding, and long-\ncontext finetuning (Zaheer et al., 2020; Beltagy\net al., 2020; Kitaev et al., 2020; Guo et al., 2022;\nWang et al., 2020a). Expanded context windows\n(up to 200K tokens) (Chen et al., 2023; OpenAI,\n2023; Mistral AI, 2024) help but still degrade in\nmulti-turn dependencies, entity tracking, and co-\nherence (Liu et al., 2024).\nChunking-Based Method Chunking-based ap-\nproaches like SLED (Ivgi et al., 2023) and Un-\nlimiformer (Bertsch et al., 2023) segment text for\nhierarchical summarization, while CachED (Sax-\nena et al., 2025) improves efficiency via gradient\ncaching but requires finetuning.\nUnlike prior methods, NEXUS SUM offers a\ntraining-free alternative leveraging Multi-LLM\nagents, allowing full text summarization without\ntruncation.\n2.3 Multi-Agent LLMs for Summarization\nRecent multi-agent LLM frameworks, such as\nChain of Agents (CoA) (Zhang et al., 2024) and\nBooookScore (Chang et al., 2024), improve docu-\nment summarization through hierarchical merging\nand sequential refinement (HM-SR) (Jeong et al.,\n2025). However, they lack adaptations for narrative\ncoherence, character interactions, and event depen-\ndencies. Retrieval-augmented generation (Lewis\net al., 2020) improves factuality but struggles with\nlong-form storytelling, often missing thematic con-\ntinuity (Geng et al., 2022; Uthus and Ni, 2023).\nNEXUS SUMaddresses these gaps by integrating\nthe dialogue-to-description transformation and sys-\ntematic length control, ensuring coherent and con-\ntextually faithful summaries."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nRevisiting Self-attention for Cross-domain Sequential Recommendation\n\n[NUGGETS]\n- Sequential recommendation is a popular paradigm in modern recommender\nsystems [ref_1]\n- In particular, one challenging problem in this space is cross-domain\nsequential recommendation (CDSR), which aims to predict future behaviors given\nuser interactions across multiple domains [ref_2]\n- Existing CDSR frameworks are mostly\nbuilt on the self-attention transformer and seek to improve by explicitly\ninjecting additional domain-specific components (e [ref_3]\n- domain-aware module\nblocks) [ref_5]\n\n[REFERENCES]\n[38] and de-\nriving domain-aware module blocks\n", "output": "2.1 Sequential Recommendation\nSequential recommendation aims at predicting user\u2019s future behav-\niors given an ordered list of user\u2019s historical interactions. Prior to\nthe popularity of transformer models, researchers explored models\nbased on recurrent architectures [ 3,50] to encode the sequential pat-\nterns in user behavior histories, such as GRU4Rec [ 13], STAMP [ 31],\nNARM [ 26], etc. These works demonstrate that models consum-\ning sequence of user behaviors significantly outperforms pair-wise\nmodels such as matrix factorization [ 39]. After the invention of\nthe transformer [ 48], sequential recommendation frameworks bydefault explore backbone model architectures based on this archi-\ntecture [ 22,44], owing to its strong capabilities of modeling long se-\nquential data that have been well demonstrated in other fields [ 1,48].\nFor instance, approaches such as SASRec [ 22], BERT4Rec [ 44],\nSINE [ 45], and LightSANs [ 7] train a transformer-based model\nwith supervision signals like causal language modeling or masked\nlanguage modeling on the user behavior sequence. Another branch\nof research explores textual attributes of behaviors (e.g., reviews\nand descriptions) and utilizes large language models to conduct\nsequential recommendation [4, 11, 16, 51, 56, 60, 65].\n2.2 Cross-domain Sequential Recommendation\nCross-domain recommendation aims at improving recommenda-\ntion performance by leveraging information from multiple domains\nsimultaneously. A branch of early studies explore matrix factoriza-\ntion approaches to model user-item interactions across different do-\nmains without considering their sequential nature [ 10,25,30,43,62].\nFollow-up research proposes cross-domain sequential recommenda-\ntion (CDSR) to further improve performance by explicitly injecting\nadditional domain-specific components, such as adding additional\nsupervision signals [2], reweighing different domains [38] and de-\nriving domain-aware module blocks [ 17,59]. Specifically, \ud835\udf0b-net\nproposes a domain-aware gating mechanism to facilitate knowl-\nedge transfer between domains [ 33]. C2DSR leverages graph neural\nnetworks that models cross-domain graphs to improve the perfor-\nmance [ 2]. Similarly, MIFN uses a knowledge graph to enhance\nCDSR [ 32]. MAN [ 27] harnesses additional supervision signals and\ndomain-aware blocks to disentangle information from different do-\nmains [ 27]. SyNCRec proposes a cooperative learning framework\nand utilizes additional domain-specific blocks to advance CDSR [ 38].\nAlthough incorporating additional components can be effective,\nsuch approaches often overlook the self-attention module in the\nbackbone transformer, which is inherently a powerful tool for cap-\nturing fine-grained correlations among heterogeneous behaviors\non its own [35, 46, 53]."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nNMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome\n  Assembly\n\n[NUGGETS]\n- De novo assembly enables investigations of unknown genomes, paving the way\nfor personalized medicine and disease management [ref_1]\n- However, it faces immense\ncomputational challenges arising from the excessive data volumes and\nalgorithmic complexity [ref_2]\n- While state-of-the-art de novo assemblers utilize distributed systems for\nextreme-scale genome assembly, they demand substantial computational and memory\nresources [ref_3]\n- They also fail to address the inherent challenges of de novo\nassembly, including a large memory footprint, memory-bound behavior, and\nirregular data patterns stemming from complex, interdependent data structures [ref_4]\n- Given these challenges, de novo assembly merits a custom hardware solution,\nthough existing approaches have not fully addressed the limitations [ref_5]\n\n[REFERENCES]\n[6] and Harp\n[10] further improved mm2-ax for long-read sequence\nmapping by running all the steps on the GPU. Additionally, Guo\net al.\n[26] accelerated the minimap2 algorithm on CPUs using\nSIMD optimizations, while mm2-ax\n[35] are widely used solutions for short-read and\nlong-read alignment, respectively.\nMultiple studies have explored accelerating these algorithms.\nmm2-fast\n[42] took a cooperative approach\nby designing a heterogeneous system involving both the CPU and\nGPU. mm2-gb\n[50] andminimap2\n", "output": "In/Near-memory processing accelerators for de novo assem-\nbly.Previous work has explored accelerating De Bruijn graph-\nbased genome assembly using in/near-memory processing. Zhou\net al. [ 57] enhanced the Megahit assembly algorithm [ 34] through\nthe use of hybrid memory cubes, exploiting high degrees of paral-\nlelism and enhanced memory bandwidth across near-data process-\ning cores. However, the Megahit algorithm does not scale well with\nincreasing genome sample data sizes, and hybrid memory cubes\nhave been discontinued since 2018.\nAngizi et al. [ 3] introduced PIM-Assembler, a processing-in-\nDRAM platform for de novo assembly. Their design achieves high\nperformance and energy efficiency through in-DRAM X(N)OR op-\nerations for accelerating comparisons and additions, along with\noptimized data partitioning and mapping techniques. However,\nwhile effective for smaller genomes (9.2 GB), their approach has\nnot been demonstrated for extremely large-scale de novo assembly\ntasks that we address in our work.\nWhile our work focuses on accelerating the Iterative Compaction\nstep, Wu et al. [ 52] accelerated the k-mer counting step using in-\nstorage processing. Their method leverages the embedded CPUcontrollers and DRAM within the solid-state drive, thereby reducing\ndata transfer between the SSD and the main memory. In contrast,\nNMP-PaK aims to speed up the Iterative Compaction step, which is\nthe most time-consuming phase.\nGPU-based de novo assembly accelerators .Several works [ 4,\n20,25,37,46] leveraged GPUs\u2019 parallel processing capabilities and\nhigh bandwidth to accelerate specific assembly operations, such as\nlocal assembly, sorting, and prefix scan.\nHowever, GPUs face significant limitations for large-scale\ngenome assembly. Their restricted onboard memory capacity [ 57]\nmakes them inadequate for extremely large datasets. Additionally,\ngenome assembly algorithms present challenges for GPUs due to\ntheir memory-intensive nature and irregular access patterns during\ngraph construction and traversal [4].\nNon-de Bruijn graph assembly accelerators .There are other\nalgorithms that researchers use when new genomes are discov-\nered. These algorithms uncover new genomes through sequence\nmapping and alignment with reference genomes. BWA-MEM [50]\nandminimap2 [35] are widely used solutions for short-read and\nlong-read alignment, respectively.\nMultiple studies have explored accelerating these algorithms.\nmm2-fast [26] accelerated the minimap2 algorithm on CPUs using\nSIMD optimizations, while mm2-ax [42] took a cooperative approach\nby designing a heterogeneous system involving both the CPU and\nGPU. mm2-gb [10] further improved mm2-ax for long-read sequence\nmapping by running all the steps on the GPU. Additionally, Guo\net al. [ 22] proposed a custom accelerator for sequence mapping us-\ning FPGA. Beyond sequence-to-sequence mapping algorithms like\nminimap2 , several accelerators targeted sequence-to-graph map-\nping of long reads, including SeGraM [6] and Harp [56].\nUnlike the above works, NMP-PaK addresses genome assembly\nwithout the need for a reference genome. In addition to performance\ngains, NMP-PaK focuses on reducing the memory footprint and\nresource requirements for large datasets.\nAccelerators for other genomic applications .MegIS [ 17] ad-\ndressed the significant data movement overhead in metagenomics\nanalysis through in-storage processing. While GenDP [ 21] and\nQUETZAL [ 39] accelerated dynamic programming algorithms\n(Smith-Waterman, Needleman-Wunsch) that are essential kernels\nin various genomic applications, including reference-guided assem-\nbly,de novo assembly, metagenomics, and sequence analysis for\nboth long and short reads, our work addresses similar fundamen-\ntal challenges of processing large, interdependent, and dynamic\ndata structures. However, we focus on end-to-end de novo assem-\nbly performance, prioritizing efficient computation with minimal\nhardware resources while handling large memory requirements."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nOptimized Text Embedding Models and Benchmarks for Amharic Passage\n  Retrieval\n\n[NUGGETS]\n- Neural retrieval methods using transformer-based pre-trained language models\nhave advanced multilingual and cross-lingual retrieval [ref_1]\n- However, their\neffectiveness for low-resource, morphologically rich languages such as Amharic\nremains underexplored due to data scarcity and suboptimal tokenization [ref_2]\n- We\naddress this gap by introducing Amharic-specific dense retrieval models based\non pre-trained Amharic BERT and RoBERTa backbones [ref_3]\n- Our proposed\nRoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17 [ref_4]\n- 6% relative\nimprovement in MRR@10 and a 9 [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Retrieval systems commonly adopt a two-stage\npipeline to optimize efficiency and effectiveness:\n(i) First-stage retrieval efficiently retrieves candi-\ndate documents using lightweight methods such as\nsparse or dense retrieval. (ii) Re-ranking refines\nthe results using computationally more intensive\nmodels, such as cross-encoders.\nSparse retrieval. Sparse retrieval is fundamental\nin IR, with BM25 known for its efficiency, inter-\npretability, and cross-domain robustness (Robert-\nson and Zaragoza, 2009). However, it strug-\ngles with vocabulary mismatch and morphological\nvariability, challenges that are particularly acute\nin morphologically rich languages like Amharic.\nLearned sparse retrieval ( LSR) methods (Formal\net al., 2021b,a) attempt to mitigate these issues\nby dynamically weighting and expanding terms,\nthereby enhancing relevance while maintaining in-\nterpretability (Dai and Callan, 2020). However,\nLSR faces limitations in low-resource settings due\nto the scarcity of annotated data, dialectal diversity,\nand morphological complexity (e.g., Amharic\u2019s\ntemplatic morphology), which necessitate subword-\naware tokenization or morphological analyzers that\nare often unavailable.\nDense retrieval. Dense retrieval encodes queries\nand documents into a shared semantic space us-\ning neural network encoders, enabling efficient re-\ntrieval via approximate nearest neighbor ( ANN )\nsearch based on embedding similarity (Johnson\net al., 2019; Karpukhin et al., 2020; Xiong et al.,\n2021). While it helps mitigate lexical mismatch, its\neffectiveness in low-resource languages is hindered\nby the need for large-scale labeled training data.\nMultilingual models such as mBERT (Pires et al.,\n2019), XLM-R (Conneau et al., 2020), and African\nlanguage-specific models like SERENGETI (Ade-\nbara et al., 2023) and AfriBERTa (Ogueji et al.,\n2021) partially address data scarcity through cross-\nlingual pretraining. However, their effectiveness in\nmorphologically complex languages like Amharic\nhas not been thoroughly investigated.Recent advances in unsupervised contrastive\nlearning, such as Contriever (Izacard et al., 2022),\nhave demonstrated strong zero-shot and multilin-\ngual retrieval performance, especially in cross-\nlingual transfer scenarios. Nonetheless, their ef-\nfectiveness in morphologically complex languages\nlike Amharic remains unexplored, as current evalu-\nations do not account for challenges arising from\nroot-based and templatic morphologies.\nBeyond data scarcity, retrieval performance is\nfurther constrained by morphological complexity\nand tokenization challenges. Amharic\u2019s templatic\nmorphology often causes standard subword tok-\nenizers to over-segment words into non-morphemic\nunits, leading to fragmented representations that\nobscure semantic relationships. Broader research\non multilingual tokenization quality (Rust et al.,\n2021) shows that excessive segmentation in mor-\nphologically rich languages introduces noise into\nsubword representations, degrading performance\nin downstream tasks.\nDespite recent advances in multilingual dense\nretrieval, state-of-the-art models such as Arctic Em-\nbed 2.0 (Yu et al., 2024) and Multilingual E5 (Wang\net al., 2024), which topped the MTEB Embedding\nLeaderboard4at the time of our study, continue\nto struggle with highly inflected languages. These\nmodels often produce suboptimal tokenizations,\nfragmented subword representations, and ineffi-\ncient embeddings, ultimately limiting their retrieval\neffectiveness. Our empirical findings in Section 6.3\nillustrate the extent to which tokenization errors\nimpair retrieval performance in Amharic.\nBridging the gap in Amharic IR. Retrieval sys-\ntems are primarily optimized for high-resource lan-\nguages, exacerbating performance disparities in\nlow-resource settings like Amharic (Nigatu and\nRaji, 2024). Prior research in Amharic IRhas ex-\nplored pre-trained embeddings (Word2Vec, fast-\nText, AmRoBERTa, Belay et al., 2021), morpholog-\nical tools (e.g., annotation frameworks, WordNet-\nbas"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nSpeaking Beyond Language: A Large-Scale Multimodal Dataset for Learning\n  Nonverbal Cues from Video-Grounded Dialogues\n\n[NUGGETS]\n- Nonverbal communication is integral to human interaction, with gestures,\nfacial expressions, and body language conveying critical aspects of intent and\nemotion [ref_1]\n- However, existing large language models (LLMs) fail to effectively\nincorporate these nonverbal elements, limiting their capacity to create fully\nimmersive conversational experiences [ref_2]\n- We introduce MARS, a multimodal language\nmodel designed to understand and generate nonverbal cues alongside text,\nbridging this gap in conversational AI [ref_3]\n- Our key innovation is VENUS, a\nlarge-scale dataset comprising annotated videos with time-aligned text, facial\nexpressions, and body language [ref_4]\n- Leveraging VENUS, we train MARS with a\nnext-token prediction objective, combining text with vector-quantized nonverbal\nrepresentations to achieve multimodal understanding and generation within a\nunified framework [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Multimodal Large Language Models. Recent\nstudies have introduced models that combine vari-\nous modalities with large language models (LLMs),\nextending their capabilities beyond text to in-\nclude visual, auditory, and multimodal reason-\ning. Specifically, to enhance visual comprehen-\nsion capabilities of LLMs, LLaV A (Liu et al.,\n2024b), Qwen-VL (Bai et al., 2023) and MiniGPT-\n4 (Chen et al., 2023) have successfully integrated\nvision encoders into pre-trained LLMs. Further-\nmore, VideoChat (Li et al., 2023) and Video-\nLLaMA (Zhang et al., 2023a) extend these ca-\npabilities to video understanding, while models\nsuch as Unified-IO-2 (Lu et al., 2024) and GPT-4-\nO (Achiam et al., 2023) expand the scope to include\nauditory modalities, showing robust multimodal\nreasoning across various inputs.\nLearning Dialogue in Video. The importance\nof analyzing conversational sentiment using mul-\ntimodal data ( e.g., text, audio, and visual) from\nvideos has driven the development of numerous\ndatasets (Busso et al., 2008; Zadeh et al., 2018;\nPoria et al., 2019). This has further spurred re-\nsearch into generating and understanding dialoguesfrom videos, leveraging multimodal cues. For in-\nstance, Champagne (Han et al., 2023) introduced\nthe YTD-18M dataset for dialogue generation us-\ning visual signals and LLMs, while MultiDia-\nlog (Park et al., 2024) combined audio and visual\ndata for generating conversations. Beyond text,\nefforts like (Shafique et al., 2023) and Emotion-\nCLIP (Zhang et al., 2023c) focus on recognizing\nnonverbal cues, such as gestures and emotions. Ad-\nditionally, works like FurChat (Cherakara et al.,\n2023) and (Lee et al., 2023) explore applying non-\nverbal signals to enhance robotic facial expres-\nsions and actions. However, existing conversational\ndatasets are often limited in scale or fail to include\ndetailed 3D facial and body language information\nnecessary for modeling nonverbal cues effectively.\nOur VENUS dataset addresses these gaps by being\nboth large-scale and scalable, offering comprehen-\nsive conversational data that integrates not only text\nbut also 3D facial expressions and body languages.\nThis enables a more nuanced understanding of non-\nverbal cues and supports the generation of richer,\ncontext-aware conversations.\nHuman Motion Synthesis in Conversation. Re-\ncent advancements in 3D human reconstruc-\ntion (Lin et al., 2023; Dwivedi et al., 2024; Dan \u02c7e\u02c7cek\net al., 2022) have significantly improved the qual-\nity of pseudo-ground truth data, providing a scal-\nable and accessible alternative to traditional sensor-\nbased methods (Yi et al., 2023). Leveraging these\ndatasets, recent works (Wu et al., 2024; Lu et al.,\n2023b) have focused on generating human motions\nfrom text. Building on this progress, our work\nutilizes pseudo labels derived from our VENUS,\nwhich addresses the lack of large-scale dataset for\nconversational settings. Unlike previous works\nlike (Ng et al., 2023, 2022), which primarily gener-\nate listener facial motions from text, our approach\nextends to produce text, facial expressions, and\nbody language, aligned with conversational con-\ntext."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nEfficient 3D Brain Tumor Segmentation with Axial-Coronal-Sagittal\n  Embedding\n\n[NUGGETS]\n- In this paper, we address the crucial task of brain tumor segmentation in\nmedical imaging and propose innovative approaches to enhance its performance [ref_1]\n- The current state-of-the-art nnU-Net has shown promising results but suffers\nfrom extensive training requirements and underutilization of pre-trained\nweights [ref_2]\n- To overcome these limitations, we integrate Axial-Coronal-Sagittal\nconvolutions and pre-trained weights from ImageNet into the nnU-Net framework,\nresulting in reduced training epochs, reduced trainable parameters, and\nimproved efficiency [ref_3]\n- Two strategies for transferring 2D pre-trained weights to\nthe 3D domain are presented, ensuring the preservation of learned relationships\nand feature representations critical for effective information propagation [ref_4]\n- Furthermore, we explore a joint classification and segmentation model that\nleverages pre-trained encoders from a brain glioma grade classification proxy\ntask, leading to enhanced segmentation performance, especially for challenging\ntumor labels [ref_5]\n\n[REFERENCES]\n[6] showing comparable perfor-\nmance, CNNs continue to dominate in medical image segmentation.\nEfficient 3D BraTS with ACS Embedding 3\nIn the existing literature, a common strategy involves laborious cros\n[15] further built upon nnU-Net\u2019s achievements. Despite transformer-based\nmethods like TransBTS\n[20] has become the de facto standard for medical im-\nage segmentation with its iconic encoder and decoder architecture. Recent BraTS\nchallenge solutions\n[21] and SwinUNetR\n", "output": "Since its inception, U-Net [20] has become the de facto standard for medical im-\nage segmentation with its iconic encoder and decoder architecture. Recent BraTS\nchallenge solutions [16,3,1] have prominently relied on CNNs using the U-shaped\nencoder-decoder design [18,14,11,15,24]. Notably, Isensee et al. secured victory\nin the BraTS2020 challenge with the nnU-Net framework [10], which features\nan automated configuration mechanism. The winning solution of the subsequent\nyear [15] further built upon nnU-Net\u2019s achievements. Despite transformer-based\nmethods like TransBTS [21] and SwinUNetR [6] showing comparable perfor-\nmance, CNNs continue to dominate in medical image segmentation.\nEfficient 3D BraTS with ACS Embedding 3\nIn the existing literature, a common strategy involves laborious cross-validation,\nassembly, and training of multiple models from scratch. To address the efficiency\nchallenge, transfer learning has garnered attention. One notable attempt in this\ndirection is Med3D [4], which offers 3D pre-trained weights. However, it is crucial\nto acknowledge that the scale of its pre-trained data remains incomparable to\nthe vast 2D natural image datasets commonly utilized in transfer learning.\nAnother approach, Model Genesis [25], leverages self-supervised methods on\n3D medical images. Although innovative, these methods have yet to surpass the\nperformance of widely explored fully-supervised approaches, which span over a\ndecade of research in the field.\nOur work posits that the full potential of efficient training and fine-tuning\nin the brain tumor segmentation problem is yet to be fully realized, particularly\nconcerning large-scale 3D pre-trained models."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nSpectral Insights into Data-Oblivious Critical Layers in Large Language\n  Models\n\n[NUGGETS]\n- Understanding how feature representations evolve across layers in large\nlanguage models (LLMs) is key to improving their interpretability and\nrobustness [ref_1]\n- While recent studies have identified critical layers linked to\nspecific functions or behaviors, these efforts typically rely on data-dependent\nanalyses of fine-tuned models, limiting their use to post-hoc settings [ref_2]\n- In\ncontrast, we introduce a data-oblivious approach to identify intrinsic critical\nlayers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered\nKernel Alignment(CKA) [ref_3]\n- We show that layers with significant shifts in\nrepresentation space are also those most affected during fine-tuning--a pattern\nthat holds consistently across tasks for a given model [ref_4]\n- Our spectral analysis\nfurther reveals that these shifts are driven by changes in the top principal\ncomponents, which encode semantic transitions from rationales to conclusions [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Representation Space Analysis Understanding\nrepresentations in neural networks (NNs) has long\nbeen a significant focus of research. Morcos et al.\n(2018) used Canonical Correlation Analysis (CCA)\nto study hidden representations, providing insights\ninto how neural networks evolve during training.\nRaghu et al. (2017) and Kornblith et al. (2019)\nintroduced Singular Vector Canonical Correlation\nAnalysis (SVCCA) and CKA, respectively, to com-\npare representations across layers and networks,\nshedding light on NN\u2019s learning dynamics. Nguyen\net al. (2021) showed blocks of contiguous hidden\nlayers with highly similar representations in large-\nTable 4: Attack Success Rate (ASR) and Harmfulness Score Evaluation Across Different Models. Results\ndemonstrate that freezing change-point layers effectively reduces the impact of attacks.\nLLaMA2-7B-Chat LLaMA2-13B-Chat Phi-3.0-Mini-128k-Instruct\nModelASR ASR Harmful ASR ASR Harmful ASR ASR Harmful\n(Keyword) (GPT) (GPT) (Keyword) (GPT) (GPT) (Keyword) (GPT) (GPT)\nMInit 2.3% 0.0% 1.04 1.67% 0.0% 1.01 12.0% 5.3% 1.32\nMFull 54.3% 35.0% 2.67 28.00% 22.00% 1.97 87.3% 74.3% 4.18\nMNon-Crit. 31.3% 17.7% 1.90 16.67% 12.67% 1.57 64.3% 51.7% 3.26\nMCrit. 17.0% 9.0% 1.47 6.00% 2.23% 1.27 51.3% 42.7% 2.85\ncapacity neural networks. Phang et al. (2021) inves-\ntigated how fine-tuning impacts the CKA similarity\npattern across layers. Liu et al. (2024a) showed that\nrepresentation consistency improves model perfor-\nmance on classification tasks. Brown et al. (2023)\napplied representation similarity metrics to explore\ngeneralization capabilities in language models. Sun\net al. (2024) analyzed how representations evolve\nacross layers and contribute to final predictions in\nLLMs. Meanwhile, Martinez et al. (2024) exam-\nined the convergence dynamics of activations by\ncomparing activation similarities across training\nsteps for each layer during the pre-train stage, of-\nfering a deeper understanding of model behavior\nacross different scales.\nCritical Layer Analysis in LLMs Transformer-\nbased large language models exhibit varied func-\ntionalities across their layers. For instance, Meng\net al. (2022) showed that middle layers predom-\ninantly encode factual information. Similarly,\nAzaria and Mitchell (2023) found that mid-depth\nlayers are crucial for capturing features essential\nfor generating trustworthy responses. Chen et al.\n(2024) observed substantial changes in the repre-\nsentation space of some layers, which can be useful\nfor model merging. Furthermore, Zhao et al. (2024)\nidentified a \"safety layer\" that correlates specific\nsafety-related behaviors to a particular layer. Jin\net al. (2025) presented how concepts emerge across\ndifferent layers from the view of concept learning.\nSkean et al. (2024) assessed the quality of activa-\ntion of these layers using various metrics, offering\ndeeper insights into internal evaluations. In this\nstudy, we investigate the representation dynamics\nof LLMs, establishing, for the first time, a connec-\ntion between layer-wise representation analysis in\npre-fine-tuned models and critical layer analysis\nin downstream fine-tuned models. Additionally,\nwe provide spectral insights into the principal com-\nponents driving change points in representation\ndynamics and examine their role in distilling ratio-nales into conclusions at these critical layers."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nUnlocking the Power of Rehearsal in Continual Learning: A Theoretical\n  Perspective\n\n[NUGGETS]\n- Rehearsal-based methods have shown superior performance in addressing\ncatastrophic forgetting in continual learning (CL) by storing and training on a\nsubset of past data alongside new data in current task [ref_1]\n- While such a concurrent\nrehearsal strategy is widely used, it remains unclear if this approach is\nalways optimal [ref_2]\n- Inspired by human learning, where sequentially revisiting tasks\nhelps mitigate forgetting, we explore whether sequential rehearsal can offer\ngreater benefits for CL compared to standard concurrent rehearsal [ref_3]\n- To address\nthis question, we conduct a theoretical analysis of rehearsal-based CL in\noverparameterized linear models, comparing two strategies: 1) Concurrent\nRehearsal, where past and new data are trained together, and 2) Sequential\nRehearsal, where new data is trained first, followed by revisiting past data\nsequentially [ref_4]\n- By explicitly characterizing forgetting and generalization error,\nwe show that sequential rehearsal performs better when tasks are less similar [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Empirical studies in CL. CL has drawn significant atten-\ntion in recent years, with numerous empirical approaches\ndeveloped to mitigate the issue of catastrophic forgetting.\nArchitecture-based approaches combat catastrophic forget-\nting by dynamically adjusting network parameters (Rusu\net al., 2016) or introducing architectural adaptations such as\nan ensemble of experts (Rype \u00b4s\u00b4c et al., 2024). Regularization-\nbased methods constrain model parameter updates to pre-\nserve the knowledge of previous tasks (Kirkpatrick et al.,\n2017; Magistri et al., 2024). Memory-based methods ad-\ndress forgetting by storing information of old tasks in the\nmemory and leveraging the information during current task\nlearning, which can be further divided into orthogonal pro-\njection based methods and rehearsal-based methods. The\nformer stores gradient information of old tasks to modify the\noptimization space for the current task (Saha et al., 2021; Lin\net al., 2022b), while the latter stores and reuses a tiny sub-\nset of representative data, known as exemplars. Exemplar\nsampling methods involve reservoir sampling (Chrysakis\n& Moens, 2020) and an information-theoretic evaluation of\nexemplar candidates (Sun et al., 2022). Other work such\nas Shin et al. (2017) retains past knowledge by replaying\n\u201dpseudo-rehearsal\u201d constructed from input data instead of\nstoring raw input. Rehearsal methods mostly use a concur-\nrent scheme that trains the model using a mix of input data\nand sampled exemplars (Chaudhry et al., 2018; Dokania\net al., 2019; Rebuffi et al., 2017; Garg et al., 2024). Other\nexemplar utilization methods include Lopez-Paz & Ranzato\n(2017) and Chaudhry et al. (2018), which use exemplar to\nimpose constraints in the gradient space.\nTheoretical studies in CL. Compared to the vast amount\nof empirical studies in CL, the theoretical understanding of\nCL is very limited but has started to attract much attention\nvery recently. Bennani & Sugiyama (2020); Doan et al.\n(2021) investigated CL performance for the orthogonal gra-\ndient descent approach in NTK models theoretically. Yin\net al. (2020) focused on regularization-based methods and\nproposed a framework, which requires second-order infor-\nmation to approximate loss function. Cao et al. (2022); Li\net al. (2022) characterized the benefits of continual represen-\n2\nUnlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective\ntation learning from a theoretical perspective. Evron et al.\n(2023) connected regularization-based methods with Pro-\njection Onto Convex Sets. Recently, a series of theoretical\nstudies proposed to leverage the tools of overparameter-\nized linear models to facilitate better understanding of CL.\nEvron et al. (2022) studied the performance of forgetting\nunder such a setup. After that, Lin et al. (2023) character-\nized the performance of CL, where they discuss the impact\nof task similarities and the task order. Ding et al. (2024)\nfurther characterized the impact of finite gradient descent\nsteps on forgetting of CL. Goldfarb & Hand (2023) illus-\ntrated the joint effect of task similarity and overparameter-\nization. Zhao et al. (2024) provided a statistical analysis\nof regularization-based methods. More recently, Li et al.\n(2024) theoretically investigated the impact of mixture-of-\nexperts on the performance of CL in linear models.\nDifferent from all these studies, we seek to fill up the theo-\nretical understanding for rehearsal-based CL. Note that one\nconcurrent study (Banayeeanzade et al., 2024) also investi-\ngates rehearsal-based CL in linear models with concurrent\nrehearsal. However, one key difference here is that we pro-\npose a novel rehearsal strategy, i.e., the sequential rehearsal,\nand theoretically show its benefit over concurrent rehearsal\nfor dissimilar tasks. Our theoretical results further motivate\na new algorithm design for CL in practice, which demon-\nstrates promising performance on DNNs."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nProbabilistic Online Event Downsampling\n\n[NUGGETS]\n- Event cameras capture scene changes asynchronously on a per-pixel basis,\nenabling extremely high temporal resolution [ref_1]\n- However, this advantage comes at\nthe cost of high bandwidth, memory, and computational demands [ref_2]\n- To address this,\nprior work has explored event downsampling, but most approaches rely on fixed\nheuristics or threshold-based strategies, limiting their adaptability [ref_3]\n- Instead,\nwe propose a probabilistic framework, POLED, that models event importance\nthrough an event-importance probability density function (ePDF), which can be\narbitrarily defined and adapted to different applications [ref_4]\n- Our approach\noperates in a purely online setting, estimating event importance on-the-fly\nfrom raw event streams, enabling scene-specific adaptation [ref_5]\n\n[REFERENCES]\n[1] while emphasizing the role of intelli-\ngent sampling.\nWe propose importance-based downsampling using\nan event-importance probability density function (ePDF),\nwhich can be arbitrarily defined and adapt\n", "output": "Modern event cameras produce an overwhelming number\nof events, posing significant challenges in bandwidth, com-putation, and memory. Efficient event stream management\nis therefore crucial, with downsampling emerging as a key\nstrategy to reduce computational and bandwidth demands.\nEarly works explored event downsampling in spatial\nand temporal dimensions by scaling event coordinates and\ntimestamps, adapting the sampling strategy to the dataset\n[5]. Later approaches refined this by integrating events over\nspace and time using a counting strategy with refractory pe-\nriods [9].\nOther methods take inspiration from biological neurons,\nreducing events based on the activation of multiple sensory\nunit layers [2]. Spiking Neural Networks (SNNs) have also\nbeen explored for downsampling, leveraging neuromorphic\nprocessing to optimize event retention [10, 12, 13, 16].\nAdaptive compression strategies, such as Huffman encod-\ning that dynamically adjusts to bandwidth constraints, have\nalso been proposed [4], along with pre-processing tech-\nniques that use non-uniform spatial sampling via 3D grids\n[3].\nBeyond computational efficiency, research has also ex-\namined how downsampling affects human perception of\nevent streams. Studies have compared basic temporal\nand spatial filtering with more advanced SNN-based ap-\nproaches to assess how sparsification impacts interpretabil-\nity [11, 12].\nDespite these advances, most existing methods rely on\nfixed heuristics or task-specific optimizations, limiting their\nadaptability across different applications.\nIn contrast to existing approaches, we formulate event\ndownsampling as an online stochastic process, where events\nare sampled based on their likelihood of belonging to an es-\ntimated event distribution. This enables adaptive selection\nbased on event importance and scene statistics, rather than\nrelying on fixed heuristics or thresholds.\nThe closest work to ours is [1], which uniformly sam-\nples events each epoch to train a CNN, studying the effects\nof event reduction and its interaction with CNN training pa-\nrameters. However, their approach does not consider event\nimportance or zero-shot applicability. Instead, we focus\non the downsampling technique itself, evaluating its perfor-\nmance across independent tasks and models trained on the\noriginal event stream. Additionally, we investigate the ef-\nfects of retraining with downsampled events, reaching sim-\nilar conclusions to [1] while emphasizing the role of intelli-\ngent sampling.\nWe propose importance-based downsampling using\nan event-importance probability density function (ePDF),\nwhich can be arbitrarily defined and adapted to different set-\ntings. To make this framework broadly applicable, we intro-\nduce a generic formulation and processing pipeline, namely\nPOLED, capable of handling any valid ePDF. In this work,\nwe present a Poisson-based ePDF designed to prioritize\ncontour preservation, under the premise that contour-related\n2\nevents are more relevant for solving diverse tasks. Further-\nmore, we approach event downsampling from a purely on-\nline perspective, making decisions based only on past and\npresent information, simulating a real-time scenario where\nfuture events are unknown.\nFinally, prior work has largely focused on sim-\nple classification datasets or introduced metrics favoring\nclassification-based evaluation. To provide a more compre-\nhensive assessment, we evaluate our method on four chal-\nlenging datasets, covering classification, frame interpola-\ntion for super-slow-motion video generation, surface nor-\nmal estimation, and object detection in an automotive set-\nting."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nNeed for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs\n\n[NUGGETS]\n- Zero-Knowledge Proofs (ZKPs) are rapidly gaining importance in\nprivacy-preserving and verifiable computing [ref_1]\n- ZKPs enable a proving party to\nprove the truth of a statement to a verifying party without revealing anything\nelse [ref_2]\n- ZKPs have applications in blockchain technologies, verifiable machine\nlearning, and electronic voting, but have yet to see widespread adoption due to\nthe computational complexity of the proving process [ref_3]\n- Recent works have\naccelerated the key primitives of state-of-the-art ZKP protocols on GPU and\nASIC [ref_4]\n- However, the protocols accelerated thus far face one of two challenges:\nthey either require a trusted setup for each application, or they generate\nlarger proof sizes with higher verification costs, limiting their applicability\nin scenarios with numerous verifiers or strict verification time constraints [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Much of the prior body of crytographic hardware and systems\nresearch has focused on Fully Homomorphic Encryption and Multi-\nParty Computation [ 19,26\u201328,37,41,47,48,54]. ZKP hardware\nresearch is relatively newer, and has focused primarily on accelerat-\ning NTTs and MSMs [ 9,11,23,25,31\u201333,35,46,59,60,63,66,67].\nA few recent works have accelerated SumChecks on GPU [ 34]\nand ASIC [ 49] as well as hashing alternatives to SHA-based hash\nfunctions [ 4,53,60]. Some systems accelerate end-to-end Groth16\nproofs (using NTTs and MSMs) on GPU [ 36] and ASICs [ 12,64].\nSZKP is presently the only ASIC that accelerates Groth16 proofs\nTable 5: Area and power of zkSpeed. Other includes the SHA3\nunit and interconnect.\nArea (mm 2) Average Power (W)\nMSM ("}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nConstrained Auto-Regressive Decoding Constrains Generative Retrieval\n\n[NUGGETS]\n- Generative retrieval seeks to replace traditional search index data\nstructures with a single large-scale neural network, offering the potential for\nimproved efficiency and seamless integration with generative large language\nmodels [ref_1]\n- As an end-to-end paradigm, generative retrieval adopts a learned\ndifferentiable search index to conduct retrieval by directly generating\ndocument identifiers through corpus-specific constrained decoding [ref_2]\n- The\ngeneralization capabilities of generative retrieval on out-of-distribution\ncorpora have gathered significant attention [ref_3]\n- In this paper, we examine the inherent limitations of constrained\nauto-regressive generation from two essential perspectives: constraints and\nbeam search [ref_4]\n- We begin with the Bayes-optimal setting where the generative\nretrieval model exactly captures the underlying relevance distribution of all\npossible documents [ref_5]\n\n[REFERENCES]\n[34] and has garnered consid-\nerable attention\n[71] also provide a Bayes-optimal\ntree structure, which is often called max-heap assumption\n", "output": "Generative retrieval ( GR)is an emerging direction in neural in-\nformation retrieval, exploring the possibility of replacing traditional\nindex structures in retrieval systems with a single large-scale neural\nnetworks [ 27,54]. It leverages generative models to directly gen-\nerate the relevant docID s given a query. This paradigm originated\nwith Cao et al . [9], Metzler et al . [34] and has garnered consid-\nerable attention [ 4,30,38,46\u201349,52,55,57,61,66,67,70] in the\ninformation retrieval community.\nGeneralization remains a challenge for GR, especially when ap-\nplied to out-of-distribution corpora [ 2,11,22,27,31,32,46]. Pre-\nvious research attributes this challenges to limited model capac-\nity [22,60], lack of learning in the docID construction [ 46,57,66],\nand difficulties in learning semantic representations [ 47,53]. In\ncontrast, our work focuses on the constrained auto-regressive de-\ncoding strategy widely applied in GR, which is crucial for adapting\nGRmodels to new corpora dynamically. Our setting aligns closely\nto the few-shot indexing approach [ 2], where a pre-trained LLM\ngenerates docID s solely based on its pre-trained knowledge and\ngeneralization capabilities, without additional training. We treat\ntheir method as a conceptual blueprint for a fully generalizable GR\nsystem and aim to investigate the inference stage under this setting.\nUpdatable generative retrieval is another critical task on dynamic\ncorpora. The primary challenges indicate the cost of updating the\nmodel with new documents and the catastrophic forgetting prob-\nlem [ 11,17,21,33]. Previous efforts have concentrated on devel-\noping efficient continual learning strategies by fixing the indexing\nconstruction procedure. We consider an idealized scenario where\nthe model has full knowledge of all possible documents and focuses\nsolely on generating relevant docIDs on dynamic corpus.\nConstrained decoding. Constrained decoding has been widely\nstudied for guiding machine learning models to produce outputs\nthat satisfy specific conditions [ 1,37]. Instead of learning to sat-\nisfy through training, constraints are more often preferable only\nin the inference time due to the flexibility and efficiency. Nishino\net al. [40,41]demonstrate the preservation of relative errors of\ncertain loss functions in realizable setting. We instead provide a\nfailure case via establishing the existence of a lower-bound error for\nauto-regressive models operating under step-wise inference-time\nconstraints. Recent work on controllable text generation ( CTG )\ninLLM s [see, e.g., 65] also explores imposing constraints during\ninference without updating the underlying model [ 10,19,35,36].\nConstrained Auto-Regressive Decoding Constrains Generative Retrieval SIGIR \u201925, July 13\u201318, 2025, Padua, Italy\nHowever, many of these approaches do not focus on strictly en-\nforcing constraint satisfaction. A few studies [ 20,63,64] propose\nmethods to produce outputs that strictly adhere to constraints,\nmainly hard keywords inclusion constraints, using tractable proba-\nbilistic models or policy gradient techniques. Our work differs by\nfocusing on a specific corpus-level constraint, i.e., the set of valid\ndocID s is sampled from the complete corpus, a problem unique to\nretrieval tasks.\nBeam search is a widely used heuristic algorithm for decoding\nstructured predictors and has been applied as a non-maximum in-\nner product search ( MIPS ) setup for large-scale retrieval systems\nwith explicit tree structures [ 24,68,69,71]. Beam search is known\nto have a performance deterioration, and only few works provided\ntheoretical insights into this issue. As far as we know, only Zhuo\net al. [71] demonstrate a training-test discrepancy in tree-structured\nmodels using binary cross-entropy loss. They showed that pseudo-\nlabeling during training does not guarantee that beam search will\nget the most relevant targets. In our work, we analyze the marginal\ndistribution of an auto-regressive distribution and provide a the-\noreti"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nTiming is important: Risk-aware Fund Allocation based on Time-Series\n  Forecasting\n\n[NUGGETS]\n- Fund allocation has been an increasingly important problem in the financial\ndomain [ref_1]\n- In reality, we aim to allocate the funds to buy certain assets within a\ncertain future period [ref_2]\n- Naive solutions such as prediction-only or\nPredict-then-Optimize approaches suffer from goal mismatch [ref_3]\n- Additionally, the\nintroduction of the SOTA time series forecasting model inevitably introduces\nadditional uncertainty in the predicted result [ref_4]\n- To solve both problems\nmentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate\n(RTS-PnO) framework, which holds no prior assumption on the forecasting models [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "2.1 Time Series Forecasting\nModern architectures for time series forecasting aim to extend\nthe forecasting horizon and improve long-term accuracy. Inspired\nby the success of Transformer-based models in capturing long-\nrange dependencies, researchers have explored various adaptations\nof the Transformer architecture for this task. These include i) re-\nducing computational complexity to sub-quadratic levels using\nsparse [ 39] and hierarchical [ 17] attention, ii) extending the at-\ntention mechanism\u2019s point-wise dependency modeling to capture\nsegment-wise [ 14] and patch-wise dependencies [ 21,37], and iii)\nmodifying the attention mechanism to incorporate domain-specific\nprocessing techniques [ 32,40]. Besides Transformer-based models,\nmodern temporal convolutional networks have also been shown to\nachieve competitive performance. MICN [ 29] combines local and\nglobal convolutions to better model long sequences, while Times-\nNet [ 31] reshapes the 1D series into 2D matrices based on salient\nperiodicities to jointly model intra-period and inter-period varia-\ntions. In fact, with the recent rise of linear models [ 36] and MLPs [ 6],\nthe de facto neural architecture for this task remains undecided. In\nthis work, we demonstrate the wide compatibility of RTS-PtO and\nRTS-PnO across various model architectures.\nOne drawback of the above-mentioned methods is the lack of\nuncertainty quantification. Existing approaches resort to generative\nmodeling [ 15,23], which naturally captures data variation. However,\nthese approaches are often limited to short-term prediction, as\nmodeling the joint data probability becomes exponentially difficult.\nAlternatively, we leverage the conformal prediction framework to\ncharacterize uncertainty for longer series [ 24,34,35], which we\nshow empirically can help achieve satisfactory performance across\ndifferent datasets.\n2.2 From PtO To PnO\nThe predict-then-optimize (PtO) can be viewed as an abstractive\nproblem for many real-world applications, such as portfolio man-\nagement or power scheduling, requiring both predicting unknown\nTiming is important: Risk-aware Fund Allocation based on Time-Series Forecasting KDD \u201925, August 3\u20137, 2025, Toronto, ON, Canada\nvalues and optimizing the target given these unknown values [ 3,5].\nSuch a paradigm has been recently extended to other large-scale\napplications, such as carrier allocation [ 33], fund recommenda-\ntion [ 27]. However, it is believed that a misalignment of targets\nexists between prediction and optimization stages. Researchers are\nincreasingly interested in training the prediction model directly\ntargeting the optimization goal, commonly known as predict-and-\noptimize (PnO) [ 12,16,28] or decision-focused learning [ 19]. The\ncore challenge is to obtain meaningful gradients for model up-\ndating, given the optimization stage. Certain researchers adopt\nanalytical approaches and aim to make the optimization layer dif-\nferentible [ 1,2]. However, these works tend to rely on strong re-\nquirements on the objective functions or constraints, restricting\ntheir application scopes in reality. Other researchers [ 7,20] instead\nadopt surrogate loss for the optimization layer and prove its con-\nvergence both theoretically and empirically. Our RTS-PnO first\nextends the application of the predict-and-optimization paradigm\nto large-scale industrial problems."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nEfficient Serverless Cold Start: Reducing Library Loading Overhead by\n  Profile-guided Optimization\n\n[NUGGETS]\n- Serverless computing abstracts away server management, enabling automatic\nscaling, efficient resource utilization, and cost-effective pricing models [ref_1]\n- However, despite these advantages, it faces the significant challenge of\ncold-start latency, adversely impacting end-to-end performance [ref_2]\n- Our study shows\nthat many serverless functions initialize libraries that are rarely or never\nused under typical workloads, thus introducing unnecessary overhead [ref_3]\n- Although\nexisting static analysis techniques can identify unreachable libraries, they\nfail to address workload-dependent inefficiencies, resulting in limited\nperformance improvements [ref_4]\n- To overcome these limitations, we present SLIMSTART,\na profile-guided optimization tool designed to identify and mitigate\ninefficient library usage patterns in serverless applications [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "cludes the paper.\nII. M OTIVATION\nThis section introduces our empirical study and its results,\nwhich motivated our work.\nA. Optimizing Library Loading: Why and How\nFig. 1: Ratio of library Initialization time to end-to-end time.\nTo quantify the impact of library initialization on overall\nend-to-end time, we evaluated a collection of serverless Python\napplications drawn from existing literature [13], [14]. Figure\n1 presents the library initialization time, end-to-end time,\nand their respective ratios. The results demonstrate that, for\nthe majority of serverless applications, library initialization\ncontributes to more than 70% of the total end-to-end time.\nThese findings highlight the critical importance of optimizing\nlibrary initialization to significantly reduce cold-start latency\nin serverless Python applications.\nObservation"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nSTAR-Rec: Making Peace with Length Variance and Pattern Diversity in\n  Sequential Recommendation\n\n[NUGGETS]\n- Recent deep sequential recommendation models often struggle to effectively\nmodel key characteristics of user behaviors, particularly in handling sequence\nlength variations and capturing diverse interaction patterns [ref_1]\n- We propose\nSTAR-Rec, a novel architecture that synergistically combines preference-aware\nattention and state-space modeling through a sequence-level mixture-of-experts\nframework [ref_2]\n- STAR-Rec addresses these challenges by: (1) employing\npreference-aware attention to capture both inherently similar item\nrelationships and diverse preferences, (2) utilizing state-space modeling to\nefficiently process variable-length sequences with linear complexity, and (3)\nincorporating a mixture-of-experts component that adaptively routes different\nbehavioral patterns to specialized experts, handling both focused\ncategory-specific browsing and diverse category exploration patterns [ref_3]\n- We\ntheoretically demonstrate how the state space model and attention mechanisms\ncan be naturally unified in recommendation scenarios, where SSM captures\ntemporal dynamics through state compression while attention models both similar\nand diverse item relationships [ref_4]\n- Extensive experiments on four real-world\ndatasets demonstrate that STAR-Rec consistently outperforms state-of-the-art\nsequential recommendation methods, particularly in scenarios involving diverse\nuser behaviors and varying sequence lengths [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Transformers and RNNs for Sequential Recommendation\nSequential recommendation has evolved significantly from tradi-\ntional methods to deep learning-based solutions [ 3,9,10,13,22,28,\n31,33,44,49,53,54,56,60]. Early approaches like TransRec [ 36]\nand matrix factorization methods [ 21] focused on modeling user-\nitem interactions through conventional data mining techniques,\nbut they struggled with capturing multiple user behaviors andfaced efficiency challenges with longer sequences. This led to the\nemergence of deep learning methods, particularly Transformers\nand RNNs. Transformer-based models like SASRec [ 18] leveraged\nmulti-head attention mechanisms for sequence modeling, while\nBERT4Rec [ 35] employed bidirectional transformers to capture\ncontextual information. LinRec [ 27] further improved efficiency\nby introducing linear complexity attention mechanisms. Despite\ntheir effectiveness, these transformer-based models suffer from qua-\ndratic computational complexity when modeling long sequences.\nRNN-based approaches like GRU4Rec [ 16] provided linear compu-\ntational complexity but showed limited effectiveness in sequential\nrecommendations. To address this, STAR-Rec combines preference-\naware attention and state-space modeling to handle variable-length\nsequences while maintaining efficiency.\nState Space Models for Sequential Recommendation Recently,\nstate-space-models (SSMs) have demonstrated remarkable effective-\nness in sequence modeling tasks due to their superior capability in\ncapturing temporal dynamics and hidden patterns [ 2,7,14,17,29,\n32,41,46\u201348,51,61]. Mamba4Rec [ 26] pioneered this direction by\ndemonstrating improved efficiency while maintaining competitive\nperformance through its selective state space modeling. Following\nthis, ECHO-Mamba4Rec [ 41] advanced the field by combining bidi-\nrectional Mamba with frequency-domain filtering for more accurate\npattern capture. RecMamba [ 47] demonstrated Mamba\u2019s capabil-\nity in handling lifelong scenarios, while Mamba4KT [ 2] adapted\nthe architecture for knowledge tracing applications. Most recently,\nSIGMA [ 29] attempted to address Mamba\u2019s limitations in context\nmodeling and short sequence handling through a bi-directional\nstructure with selective gating mechanisms. These approaches face\nchallenges in balancing long/short-term sequence modeling and pat-\ntern diversity in recommendation scenarios. STAR-Rec addresses\nthis through preference-aware attention and state-space modeling\nvia sequence-level mixture-of-experts, effectively handling diverse\nitem relationships and varying sequence patterns."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nVDDP: Verifiable Distributed Differential Privacy under the\n  Client-Server-Verifier Setup\n\n[NUGGETS]\n- Despite differential privacy (DP) often being considered the de facto\nstandard for data privacy, its realization is vulnerable to unfaithful\nexecution of its mechanisms by servers, especially in distributed settings [ref_1]\n- Specifically, servers may sample noise from incorrect distributions or generate\ncorrelated noise while appearing to follow established protocols [ref_2]\n- This work\nanalyzes these malicious behaviors in a general differential privacy framework\nwithin a distributed client-server-verifier setup [ref_3]\n- To address these adversarial\nproblems, we propose a novel definition called Verifiable Distributed\nDifferential Privacy (VDDP) by incorporating additional verification\nmechanisms [ref_4]\n- We also explore the relationship between zero-knowledge proofs\n(ZKP) and DP, demonstrating that while ZKPs are sufficient for achieving DP\nunder verifiability requirements, they are not necessary [ref_5]\n\n[REFERENCES]\n[14] \u2714 \u2714 \u2714 \u2714 \u2714 \u2718\nOurs \u2714 \u2714 \u2714 \u2714 \u2714 \u2714\n(MPC)\n[61] \u2714 \u2714 \u2714 \u2714 \u2718 \u2718\nSTC+24\n[63] \u2714 \u2718 \u2718 \u2714 \u2714 \u2718\nKCY21\n[74] \u2714 \u2714 \u2718 \u2718 \u2718 \u2718\nDPrio\n[84] \u2714 \u2714 \u2714 \u2718 \u2718 \u2718\nVDBM\n", "output": "Pioneering steps in verifiable executions of differentially pri-\nvate mechanisms involve cryptographic proofs on the cor-\nrectness of deterministic fundamental computation steps in\ndifferentially private database systems like VFuzz [74] and\nDPrio [63]. More recent advancements have shifted their\nfocus to the correct sampling from noise distributions, in-\ncluding randomized response ( KCY21 , [61]), floating-point\nGaussian mechanisms ( STC+24 , [84]), and binomial mech-\nanisms ( VDBM , [14]). More broadly, other studies on se-\ncure computation for randomness generation [3, 20] and\ndifferential privacy [10, 29], with multi-party computationTable 5: Comparison of security and privacy models with\nprevious work on MPCs of DP mechanisms ( MPC-DP ,\n[17, 18, 24, 36, 91]) and verifiable executions of DP mech-\nanisms [14, 61, 63, 74, 84]. VD,VC,VR: authenticity of data,\ncorrect deterministic computation, or correct sampling from\nthe prescribed random distributions is verifiable (to an exter-\nnal data analyst); N: resilience against numerical issues of DP\ndue to compatibility with discrete cryptographic primitives;\nCSV: client-server-verifier model; E2EDP : end-to-end DP\nguarantee, incorporating additional leakages from the proof.\nVD VC VR N CSV E2EDP\nMPC-DP \u2718 \u2718 \u2718 \u2714 \u2718 N/A\nVFuzz [74] \u2714 \u2714 \u2718 \u2718 \u2718 \u2718\nDPrio [63] \u2714 \u2718 \u2718 \u2714 \u2714 \u2718\nKCY21 [61] \u2714 \u2714 \u2714 \u2714 \u2718 \u2718\nSTC+24 [84] \u2714 \u2714 \u2714 \u2718 \u2718 \u2718\nVDBM [14] \u2714 \u2714 \u2714 \u2714 \u2714 \u2718\nOurs \u2714 \u2714 \u2714 \u2714 \u2714 \u2714\n(MPC) [17, 18, 24, 34, 36, 44, 91], have laid the foundation for\nthe secure computation of DP mechanisms, especially in dis-\ntributed settings. However, despite the similarities in multiple\naspects, they do not cover the scenario when an external data\nanalyst needs to verify the authenticity of the data and cor-\nrectness of computation, especially the randomness involved.\nWe compare this study\u2019s security and privacy models with\nthe aforementioned studies in Table"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nKVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider\n\n[NUGGETS]\n- Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency [ref_1]\n- However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent [ref_2]\n- In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers [ref_3]\n- We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate [ref_4]\n- Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Reuse KV$ across requests via KV$ cache. Reusing KV$\nfor accelerating LLM serving has been widely studied [ 30,63,\n19,20,69,62] and used in commercial LLM serving [ 42,21,\n5,2]. Currently, production systems only treat cache hits by\nprefix matches [ 63,69,19,42,21,5,2], because it preserves\nthe original algorithm of model inference with no accuracy\nloss. A number of studies [ 20,62,23] have studied methods\nfor non-prefix KV$ cache. We can revisit our characterization\nonce they have matured and been deployed in production.\nOther KV$ -related optimizations. Emerging works[ 59,68,\n33,13] propose runtime KV$ compression and deletion meth-\nods to reduce KV$ size. StreamingLLM [ 59] keeps only a\nfinite attention window for recent tokens combined with a few\ninitial tokens. H2O [ 68] only involves tokens that contribute\nmost to the attention score for inference. InfiniGen [ 33] spec-\nulatively selects tokens critical for attention scores and drops\nothers. KVQuant [ 22] enables 3-bit KV cache quantizationwith minimal perplexity degradation for large language mod-\nels, achieving fast million-length context inference. These\nmethods require fewer KV$ per-request, albeit with accuracy\ndegradation. Our study is compatible with these works: if an\nuncompressed/full KV$ is reusable, a compressed or deleted\nversion would also be reusable.\nOptimizing caching policies. Optimizing caching has long\nbeen studied in the literature, in general-purpose caching\npolicies [ 41,29,28,53,32,25,38,71,10,24,16,11,49,\n67,14] or specific domains [ 60,12,50,17]. We continue this\nline of research, and leverage our characterized KV$ reuse\nproperties for optimizing KV$ cache policies.\nOptimizing LLM serving. We continue the line of re-\nsearch in optimizing the performance of LLM serving sys-\ntems [ 70,57,31,34,64,46,18,66], with a particular focus\non characterizing serving workloads for KV$ cache. Our\nwork is orthogonal to optimizations other than KV$ cache."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nUnconstrained Monotonic Calibration of Predictions in Deep Ranking\n  Systems\n\n[NUGGETS]\n- Ranking models primarily focus on modeling the relative order of predictions\nwhile often neglecting the significance of the accuracy of their absolute\nvalues [ref_1]\n- However, accurate absolute values are essential for certain downstream\ntasks, necessitating the calibration of the original predictions [ref_2]\n- To address\nthis, existing calibration approaches typically employ predefined\ntransformation functions with order-preserving properties to adjust the\noriginal predictions [ref_3]\n- Unfortunately, these functions often adhere to fixed\nforms, such as piece-wise linear functions, which exhibit limited\nexpressiveness and flexibility, thereby constraining their effectiveness in\ncomplex calibration scenarios [ref_4]\n- To mitigate this issue, we propose implementing\na calibrator using an Unconstrained Monotonic Neural Network (UMNN), which can\nlearn arbitrary monotonic functions with great modeling power [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "In this section, we examine existing research on the calibration\nof ranking systems in recommendation and search. The research\ncan be classified into two categories based on the primary focus:\ncalibrator modeling and loss reconciling.\n2https://www.kuaishou.com\nUnconstrained Monotonic Calibration of Predictions in Deep Ranking Systems SIGIR \u201925, July 13\u201318, 2025, Padua, Italy\n2.1 Calibrator Modeling\nThe primary research focus lies in calibrator modeling, with the\nobjective of developing powerful calibrators to refine original pre-\ndictions. Early methods predominantly leverage the statistical char-\nacteristics of original predictions to perform univariate transforma-\ntions that ensure global order preservation. Specifically, binning\nmethods [ 21,41] involve partitioning samples into bins and ad-\njusting each sample by assigning a statistical quantity, such as the\naverage predicted probability of its bin, as the calibrated value. Iso-\ntonic regression methods [ 13,42] optimize squared errors under\na non-decreasing constraint to fit a univariate isotonic calibration\nfunction. Scaling methods [ 16,24,26,33,46] directly fit predefined\ntransformations, like the logistic function, for calibration purposes.\nIn general, the global order preservation of these methods theoreti-\ncally prevents the calibration process from affecting the ranking\nperformance. However, the constrained parameter space of these\ntransformations limits their effectiveness, particularly in the con-\ntext of industrial deep ranking models [8].\nLater works delve into the exploration of utilizing neural net-\nworks for calibration, using them to adaptively learn parameters\nof transformation functions for samples with varying features. For\ninstance, FAC [ 30] combines a univariate piece-wise linear model\nwith a field-aware auxiliary neural network. AdaCalib [ 37] learns\nisotonic function families to calibrate predictions, guided by pos-\nterior statistics. SBCR [ 44] introduces a neural piece-wise linear\nmodel that integrates sample features directly into the learning of\nlinear weights. However, due to the inadequate fitting performance\nof piece-wise linear interpolation [ 9], these methods struggle to\nthoroughly handle multi-field calibration that involves nuanced\npatterns. DESC [ 40], developed concurrently with our approach,\nreplaces piece-wise linear functions with combinations of multiple\nnonlinear basis functions. Nevertheless, the calibrator expressive-\nness remains partially constrained, as it lacks guarantees for fitting\narbitrary monotonic functions.\n2.2 Loss Reconciling\nApart from the calibrator modeling aspect, there is also research\nfocused on designing joint optimization strategies to handle point-\nwise calibration loss and pairwise or list-wise ranking loss, aiming\nto enhance compatibility between ranking and calibration.\nSpecifically, CalSoftmax [ 38] addresses training divergence is-\nsues of the ranking loss and achieves calibrated outputs through the\nuse of virtual candidates. JRC [ 34] utilizes two logits for click and\nnon-click states to decouple the optimization of ranking and calibra-\ntion. RCR [ 2] proposes a regression-compatible ranking approach\nto balance calibration and ranking accuracy. CLID [ 15] employs a\ncalibration-compatible list-wise distillation loss to distill the teacher\nmodel\u2019s ranking ability without destroying the model\u2019s calibration\nability. SBCR [ 44] introduces a self-boosted ranking loss that uti-\nlizes dumped ranking scores obtained from the online deployed\nmodel, facilitating comparisons between samples associated with\nthe same query and allowing for extensive shuffling of sample-level\ndata. BBP [ 28] tackles the issue of insufficient samples for ranking\nloss by estimating beta distributions for users and items, generat-\ning continuously comparable ranking score labels. However, theyprimarily focus on reconciling the optimization of ranking and\ncalibration, instead of developing calibrator architectures."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nPre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval\n  Knowledge Acquisition\n\n[NUGGETS]\n- Dense retrievers utilize pre-trained backbone language models (e [ref_1]\n- , BERT,\nLLaMA) that are fine-tuned via contrastive learning to perform the task of\nencoding text into sense representations that can be then compared via a\nshallow similarity operation, e [ref_3]\n- inner product [ref_5]\n\n[REFERENCES]\n[2016] and Natural Questions\n(NQ) Kwiatkowski et al.\n[2019] are commonly used to evaluate Dense retrievers. MS MARCO consists of\n17\nweb search queries with passage relevance annotations, while NQ features real-world questions paired with\nrelevant Wikipedia pas\n[2021] integrates prompt-based learning with dense retrieval, employing carefully designed prompts\nto guide the model in generating more discriminative representations. These approaches illustrate the growin\n", "output": "Dense retrieval relies on deep neural networks to encode textual data into dense vector representations,\nenabling efficient approximate nearest neighbor search. These models are broadly categorized into encoder-\nbased and decoder-based retrievers, each employing different representation and retrieval strategies.\nEncoder-based dense retrievers , such as DPR Karpukhin et al. [2020], utilize transformer encoders to\nmap queries and documents into fixed-size vectors. The retrieval process is then performed by computing the\ndot product between query and document representations. One common approach is the use of the [CLS]\ntoken representation , where the final hidden state of the special [CLS] token is extracted as a compact\nrepresentation of the entire input sequence. While effective, this method has been observed to focus more on\nthe beginning of the input, potentially missing finer-grained information distributed throughout the text.\nAn alternative strategy is mean pooling , as adopted by Contriever Izacard and Grave [2021], where\nthe final embeddings of all tokens are averaged to form a unified document representation. This pooling\nmechanism captures a more distributed representation of the input and is often preferred in cases where\ninformation is spread across longer passages. Furthermore, some models, such as Sentence-BERT Reimers\nand Gurevych [2019] and SimCSE Gao et al. [2021], extend dense retrieval capabilities to zero-shot settings,\nleveraging contrastive learning and pre-trained embeddings to provide robust document representations\nwithout requiring task-specific fine-tuning.\nDecoder-based dense retrievers are in contrast to encoder-based models, incorporating autoregressive\ndecoding mechanisms for retrieval. RePLAMA Smith and Doe [2021] exemplifies this approach by reframing\nthe retrieval task as a sequence generation problem, where the model generates candidate passages based on\nquery context rather than performing direct similarity matching. This method enhances retrieval flexibility by\ncapturing long-range dependencies and richer query-document relationships. Similarly, PromptReps Lee and\nKumar [2021] integrates prompt-based learning with dense retrieval, employing carefully designed prompts\nto guide the model in generating more discriminative representations. These approaches illustrate the growing\nshift toward generative retrieval frameworks that combine aspects of traditional retrieval with neural sequence\nmodeling.\nBenchmarks such as MS MARCO Passage Ranking Nguyen et al. [2016] and Natural Questions\n(NQ) Kwiatkowski et al. [2019] are commonly used to evaluate Dense retrievers. MS MARCO consists of\n17\nweb search queries with passage relevance annotations, while NQ features real-world questions paired with\nrelevant Wikipedia passages. These datasets provide diverse and realistic challenges, making them essential\nfor assessing retrieval effectiveness and advancing research in dense retrieval."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nCLONE: Customizing LLMs for Efficient Latency-Aware Inference at the\n  Edge\n\n[NUGGETS]\n- Deploying large language models (LLMs) on edge devices is crucial for\ndelivering fast responses and ensuring data privacy [ref_1]\n- However, the limited\nstorage, weight, and power of edge devices make it difficult to deploy\nLLM-powered applications [ref_2]\n- These devices must balance latency requirements with\nenergy consumption and model accuracy [ref_3]\n- In this paper, we first quantify the\nchallenges of deploying LLMs on off-the-shelf edge devices and then we present\nCLONE, an in-depth algorithm-hardware co-design at both the model- and\nsystem-level that intelligently integrates real-time, energy optimization while\nmaintaining robust generality [ref_4]\n- In order to maximize the synergistic benefits of\nthese algorithms in always-on and intermediate edge computing settings, we\nspecialize in a 28nm scalable hardware accelerator system [ref_5]\n\n[REFERENCES]\n[125] and often causing Out-of-Memory\n(OOM) errors.\nTable 1: Popular ML hardware specifications.\nGPU Types Peak Perf. Memory Bandwidth Peak Power\nServer-level\nNVIDIA A100\n[132] adopts\nUsenix\u202fATC\nis no more?\nLLM\nIteration 1\nLLM\nIteration 2LLM\nIteration 3LLM\nIteration 4\nToken Generation/Decoding Prompt/PrefillKV\nCacheKV\nCacheKV\nCache\nLegacy\nstill\nlives EOSFigure 1: Overview of\n", "output": "2.1 Large Language Models\nLLM Architecture. In contrast to traditional deep neu-\nral networks (DNNs) and convolutional neural networks\n(CNNs) [38, 76], which integrate diverse types of layers (e.g.,\nconvolutional (CONV), fully connected (FC), recurrent (RC),\npooling, etc.) designed for specific tasks, large language mod-\nels (LLMs) predominantly consist of a uniform stack of trans-\nformer decoder layers. For instance, Llama-7B [132] adopts\nUsenix\u202fATC\nis no more?\nLLM\nIteration 1\nLLM\nIteration 2LLM\nIteration 3LLM\nIteration 4\nToken Generation/Decoding Prompt/PrefillKV\nCacheKV\nCacheKV\nCache\nLegacy\nstill\nlives EOSFigure 1: Overview of LLMs autoregressive inference.\na homogeneous architecture composed of 32 identical Lla-\nmaDecoderLayers. Each decoder layer encompasses two core\ncomponents: LlamaAttention and LlamaMLP. Despite the\nstructural uniformity across decoder layers, their contribu-\ntions to model efficiency and effectiveness vary significantly\n[26, 137]. Consequently, optimizing the inference execution\nof LLMs necessitates a detailed analysis of the individual\nimpact of each layer (\u00a73 .1).\nLLMs Inference. LLMs process a structured sequence in-\nvolving multiple forward passes through the model to se-\nquentially generate each output token. Figure 1 shows the\ninference process with a simple example. Typically, this pro-\ncess mainly contains two stages [3,75,118]. 1) Pre-fill takes a\nprompt sequence and generates the key-value (KV) cache for\neach Transformer layer of LLM. Upon receiving the prompt\n\u201cUsenix ATC is no more?\u201d , the tokenizer embeds input as to-\nkens, denoted as Xin\u2208Rn\u00d7d, where dis the hidden size and n\nis the length of input token. Then, the LLM handles all input\ntokens in parallel during a single forward iteration to gener-\nate a KV cache. The output of attention is sent to MLP to\ngenerate the first output token \u201cLegacy\u201d . Large-scale matrix\nmultiplications are required to generate the KV cache, which\nmakes the pre-fill computing intensive. 2) Decoding utilizes\nand updates the KV cache to generate tokens step-by-step.\nFollowing the generation of the first token, the LLM leverages\nthe KV caches prepared earlier and adds new information to\nthem. The creation of each new token is influenced by the to-\nkens generated before it. During each token generation, for the\ninput Xdec\u2208R1\u00d7d, attention layers load the previously stored\nKV cache, and new KV pairs are computed and concatenated\nto the existing cache. The output of the last decoder layer\nis sent to the final prediction layer to predict the next token\nsequentially. It executes iteratively until an End of Sequence\n(EOS) token is encountered or a predefined termination crite-\nrion is met. Unlike traditional models with fixed input formats\nand structured workflows [38,76], LLM inputs and outputs are\nhighly non-deterministic. This stems from the diverse, open-\nended nature of user prompts, which vary widely in structure,\nintent, and context [15, 87, 89]. Additionally, autoregressive\ntoken generation is inherently probabilistic, driven by sam-\npling methods [50, 111, 115, 132] and variability in training\ndatasets [11, 13, 134], making outputs context-sensitive.2.2 Bottlenecks of Deploying Edge LLMs\nTable 1 lists specifications for server-level and edge-level\nprocessors commonly used for ML workloads, highlighting\nresource collapse. Despite the potential benefits, including\nprivacy preservation and instant responses without depending\non a stable internet connection [42, 105], deploying LLMs on\nthe edge faces the following critical bottlenecks.\n1) High Memory Footprint. The main contributors of\n\u201cbillion-parameter\u201d LLMs are model weights (memory is\noccupied by the model parameters) and KV cache (mem-\nory is occupied by the caching of self-attention tensors\nto avoid redundant computation). For example, Llama-7B\nin 16-bit precision requires approximately 14GB memory\n(7B\u00d7sizeof(FP16) ). Its architecture with 32 layers, 32 heads\nper layer, and a head dimension of 128 incurs a mem"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nCOSMIC: Generalized Refusal Direction Identification in LLM Activations\n\n[NUGGETS]\n- Large Language Models (LLMs) encode behaviors such as refusal within their\nactivation space, yet identifying these behaviors remains a significant\nchallenge [ref_1]\n- Existing methods often rely on predefined refusal templates\ndetectable in output tokens or require manual analysis [ref_2]\n- We introduce\n\\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an\nautomated framework for direction selection that identifies viable steering\ndirections and target layers using cosine similarity - entirely independent of\nmodel outputs [ref_3]\n- COSMIC achieves steering performance comparable to prior methods\nwithout requiring assumptions about a model's refusal behavior, such as the\npresence of specific refusal tokens [ref_4]\n- It reliably identifies refusal directions\nin adversarial settings and weakly aligned models, and is capable of steering\nsuch models toward safer behavior with minimal increase in false refusals,\ndemonstrating robustness across a wide range of alignment conditions [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Our work builds on research in LLM safety and\nmechanistic interpretability.\nSafety: LLM alignment is typically achieved\nthrough fine-tuning (Ouyang et al., 2022) and\nRLHF (Bai et al., 2022; Ganguli et al., 2022), yet\nstudies show that fine-tuning (Lermen et al., 2023;\nYang et al., 2023; Qi et al., 2024) and adversarial\nprompts (Andriushchenko et al., 2024; Zou et al.,\n2023b; Chao et al., 2023) can bypass refusal mech-\nanisms.\nSteering: Recent work demonstrate refusal be-\nhavior is encoded in activation space (Weidinger\net al., 2021; Arditi et al., 2024; Marshall et al.,\n2024) with interventions aiming to modulate it di-\nrectly (Zou et al., 2023a; Arditi et al., 2024; Mar-\nshall et al., 2024; Qiu et al., 2024; Bhattacharjee\net al., 2024; Uppaal et al., 2025). Many methods\nuse contrastive data pairs to extract feature direc-\ntions (Burns et al., 2023; Arditi et al., 2024; Pan-\nickssery et al., 2023; Zou et al., 2023a) for behavior\nsteering (Zou et al., 2023a; Panickssery et al., 2023;\nTurner et al., 2023; Arditi et al., 2024; Lee et al.,\n2024) and concept removal techniques (Guerner\net al., 2023; Haghighatkhah et al., 2022; Ravfo-\ngel et al., 2020; Belrose et al., 2023) such as Rep-\nresentation Engineering and Contrastive Activa-\ntion Addition (Zou et al., 2023a; Panickssery et al.,\n2023).Wang and Shu (2023) also uses similarity-based scores to target intervention layers.\nInterpretability : Model behaviors are often rep-\nresented as linearly encoded in activation space\n(Bolukbasi et al., 2016; Elhage et al., 2022; Park\net al., 2024; Mikolov et al., 2013; Nanda et al.,\n2023; Hernandez and Andreas, 2021), although\nother work posit refusal behaviors as affine func-\ntions (Marshall et al., 2024). These hypothesis\nare investigated via mechanistic interpretability ap-\nproaches leveraging sparse autoencoders (Bricken\net al., 2023; Templeton et al., 2024; Huben et al.,\n2024), weight-based analysis (Pearce et al., 2024),\nand circuit analysis (Elhage et al., 2021; Lieberum\net al., 2023) to further understand model internals."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nTest-time Vocabulary Adaptation for Language-driven Object Detection\n\n[NUGGETS]\n- Open-vocabulary object detection models allow users to freely specify a class\nvocabulary in natural language at test time, guiding the detection of desired\nobjects [ref_1]\n- However, vocabularies can be overly broad or even mis-specified,\nhampering the overall performance of the detector [ref_2]\n- In this work, we propose a\nplug-and-play Vocabulary Adapter (VocAda) to refine the user-defined\nvocabulary, automatically tailoring it to categories that are relevant for a\ngiven image [ref_3]\n- VocAda does not require any training, it operates at inference\ntime in three steps: i) it uses an image captionner to describe visible\nobjects, ii) it parses nouns from those captions, and iii) it selects relevant\nclasses from the user-defined vocabulary, discarding irrelevant ones [ref_4]\n- Experiments on COCO and Objects365 with three state-of-the-art detectors show\nthat VocAda consistently improves performance, proving its versatility [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Open-vocabulary object detection (OvOD) [ 11] aims to map\npredicted region features to a frozen vision-language embed-\nding space, typically from contrastive models like CLIP [ 4].\nOvOD detectors usually train on box-labeled data [ 9,12] with\nlimited categories due to high annotation costs, and supplement\nthese with datasets annotated at image level [ 13], which cover\nmore classes. Major studies have focused on improving align-\nment training via pseudo-labeling [ 5], transfer learning [ 14],\nor enhanced weak supervision [ 8]. In contrast, we improve\noff-the-shelf OvOD detectors without fine-tuning, updating\nonly their vocabularies. Our work relates to SHiNe [ 15], which\naugments vocabularies via prompt engineering and a semantic\nhierarchy, but produces a single improved vocabulary for all\nimages. Instead, V ocAda adapts the vocabulary per image at\ntest time, complementing prompt-engineering approaches that\ncan further augment V ocAda\u2019s refined vocabulary"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nSearch-Based Interaction For Conversation Recommendation via Generative\n  Reward Model Based Simulated User\n\n[NUGGETS]\n- Conversational recommendation systems (CRSs) use multi-turn interaction to\ncapture user preferences and provide personalized recommendations [ref_1]\n- A\nfundamental challenge in CRSs lies in effectively understanding user\npreferences from conversations [ref_2]\n- User preferences can be multifaceted and\ncomplex, posing significant challenges for accurate recommendations even with\naccess to abundant external knowledge [ref_3]\n- While interaction with users can clarify\ntheir true preferences, frequent user involvement can lead to a degraded user\nexperience [ref_4]\n- To address this problem, we propose a generative reward model based simulated\nuser, named GRSU, for automatic interaction with CRSs [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Our work is related to the following two research directions.\nConversational recommendation. Conversational recommender\nsystems (CRSs) aim to provide item recommendations through\nmulti-turn interaction. One line of work [ 20,21,23] focuses on the\noptimization of interaction policy. They simplify the interaction to\npre-defined actions ( e.g.,asking questions or making recommen-\ndations) and handcrafted templates. Based on this, they optimize\nCRSs to give accurate recommendations within as few turns as\npossible. Another line of work [ 32,33,42] focuses on the elicitation\nand understanding of user preference in more free-form natural\nlanguage conversations. Since conversations usually lack sufficient\ncontextual information, existing work introduces knowledge from\nexternal resources, such as knowledge graphs [ 8], large language\nmodels (LLMs) [ 13,36], and conversational recommendation cor-\npora [ 8,34]. Our work follows the second category and proposes a\nsimulated user, which can automatically interact with CRSs to help\nthem discern the true user preference from a complex conversation.\nGenerative reward models. Reward models [ 7,25] have become\nan emerging topic for solving complex reasoning tasks. For example,\nin the commonly used \u201cBest-of-N\u201d strategy [ 7], a task model first\ngenerates several candidate solutions, then a reward model ranks\nthese candidates and selects the best one as the final prediction.\nRecently, generative reward models [ 26,38] have been proposed,\nwhich unify generation and reward modeling by representing re-\nward as the probability of a specific token. Based on this, critiques\ncan be introduced in generation for better reward modeling [ 26,38].\nIn this work, we take inspiration from generative reward models to\ndesign the actions of our simulated user."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nMulti-Objective Memory Bandwidth Regulation and Cache Partitioning for\n  Multicore Real-Time Systems\n\n[NUGGETS]\n- Memory bandwidth regulation and cache partitioning are widely used techniques\nfor achieving predictable timing in real-time computing systems [ref_1]\n- Combined with\npartitioned scheduling, these methods require careful co-allocation of tasks\nand resources to cores, as task execution times strongly depend on available\nallocated resources [ref_2]\n- To address this challenge, this paper presents a 0-1\nlinear program for task-resource co-allocation, along with a multi-objective\nheuristic designed to minimize resource usage while guaranteeing schedulability\nunder a preemptive EDF scheduling policy [ref_3]\n- Our heuristic employs a multi-layer\nframework, where an outer layer explores resource allocations using\nPareto-pruned search, and an inner layer optimizes task allocation by solving a\nknapsack problem using dynamic programming [ref_4]\n- To evaluate the performance of the\nproposed optimization algorithm, we profile real-world benchmarks on an\nembedded AMD UltraScale+ ZCU102 platform, with fine-grained resource\npartitioning enabled by the Jailhouse hypervisor, leveraging cache set\npartitioning and MemGuard for memory bandwidth regulation [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "In this section, we give an overview of the related works on task and resource allocation\nstrategies for real-time systems and discuss the differences between our proposed algorithm\nand the state-of-the-art resource-task co-allocation methods.\n2.1 Task Allocation\nMapping tasks statically to individual processors is widely used in industry practice due to its\nlow scheduling overhead [12]. However, since the task allocation problem is NP-hard in the\nstrongsense[23], manyapproximationmethodshavebeendevelopedforbothpreemptive[9,14,\n21,40] and non-preemptive [25,54] scheduling policies. These methods have also been extended\nto support parallel task scheduling, including directed acyclic graphs (DAGs) [15,26,68] and\ngang tasks [58,60,62], as well as to take into account inter-task interference [69]. On the\nother hand, exact approaches to the partitioning problem use optimization techniques such\nas mixed-integer linear programming (MILP) [1,47]. While MILP formulations can provide\nexact solutions, their scalability remains a challenge, particularly in systems with a large\nnumber of tasks or processors. A detailed discussion on the precise complexity classes of a\nlist of real-time task allocation problems can be found in [23].\n2.2 Resource Allocation\nCache and memory bandwidth are two critical resources to be partitioned for achieving timing\npredictability in real-time systems. A widely adopted software-based approach to cache\npartitioning is cache coloring , which has been implemented at both the operating system\n(OS) [33,34,42] and hypervisor levels [35,64]. In this paper, we rely on a cache-coloring\nimplementation available in the Jailhouse hypervisor [45]. Alternatively, caches can also be\npartitioned via hardware modifications ( e.g., [17]) or by exploiting hardware support such as\nthe Arm DSU [6], which is only available on very recent embedded Arm platforms and notably\nnot yet supported on our Ultrascale+. Similarly to caches, memory bandwidth partitions\nECRTS 2025\n7:"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nWhat About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile\n  App Reviews\n\n[NUGGETS]\n- Opinion mining plays a vital role in analysing user feedback and extracting\ninsights from textual data [ref_1]\n- While most research focuses on sentiment polarity\n(e [ref_2]\n- , positive, negative, neutral), fine-grained emotion classification in app\nreviews remains underexplored [ref_4]\n- This paper addresses this gap by identifying and\naddressing the challenges and limitations in fine-grained emotion analysis in\nthe context of app reviews [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n", "output": "exists. Riccosan published an Indonesian dataset of app re-\nviews annotated with Parrott\u2019s taxonomy [36]. While relatively\nlarge (20K reviews), it is not available in English. Moreover,\ntheir annotation relied on two annotators, reporting a Cohen\u2019s\nKappa of 0"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nAligning Web Query Generation with Ranking Objectives via Direct\n  Preference Optimization\n\n[NUGGETS]\n- Neural retrieval models excel in Web search, but their training requires\nsubstantial amounts of labeled query-document pairs, which are costly to\nobtain [ref_1]\n- With the widespread availability of Web document collections like\nClueWeb22, synthetic queries generated by large language models offer a\nscalable alternative [ref_2]\n- Still, synthetic training queries often vary in quality,\nwhich leads to suboptimal downstream retrieval performance [ref_3]\n- Existing methods\ntypically filter out noisy query-document pairs based on signals from an\nexternal re-ranker [ref_4]\n- In contrast, we propose a framework that leverages Direct\nPreference Optimization (DPO) to integrate ranking signals into the query\ngeneration process, aiming to directly optimize the model towards generating\nhigh-quality queries that maximize downstream retrieval effectiveness [ref_5]\n\n[REFERENCES]\n[6] uses task-specific prompts to perform\nfew-shot query generation. Building on these approaches, the Gecko\nmodel\n", "output": "Transformer-based bi-encoders are the standard architecture for\ndense retrieval [ 16,38], typically trained with contrastive objectives\nand hard negative mining strategies such as ANCE [ 38]. Retrieval-\naligned pre-training on in-domain corpora is also commonly adopted\nto improve retrieval effectiveness [9, 19\u201322, 36].\nThe community has explored generating synthetic queries from\ndocuments for model training. Approaches such as Doc2Query [ 25]\nand DocT5Query [ 24] train a lightweight Transformer [ 33] on la-\nbeled query\u2013document pairs to expand document representations.\nSubsequent work [ 10] demonstrates that filtering out hallucinated\nqueries can further enhance downstream performance. More recent\nmethods leverage LLMs. For instance, InPars [ 2] employs few-shot\nprompting with filtering based on generation probability, while\nInPars-v2 [ 15] leverages a supervised ranker for query filtering.\nSimilarly, Promptagator [6] uses task-specific prompts to perform\nfew-shot query generation. Building on these approaches, the Gecko\nmodel [ 18] iteratively refines synthetic queries through a process in-\nvolving retrieval, re-ranking, positive relabeling, and hard negative\nsampling, thereby improving the quality of the training data.\nRecent work has explored reinforcement learning to improve\nsynthetic query generation. Token-level Proximal Policy Optimiza-\ntion (TPPO) [ 26,30] has been applied to query suggestion tasks,\noptimizing generation based on token-level rewards derived from\nuser interaction histories. While this setting targets interactive sce-\nnarios rather than offline generation from documents, it highlights\nthe promise of reinforcement learning for enhancing query quality."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nSPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model\n\n[NUGGETS]\n- Inspired by the success of unsupervised pre-training paradigms, researchers\nhave applied these approaches to DNA pre-training [ref_1]\n- However, we argue that these\napproaches alone yield suboptimal results because pure DNA sequences lack\nsufficient information, since their functions are regulated by genomic profiles\nlike chromatin accessibility [ref_2]\n- Here, we demonstrate that supervised training for\ngenomic profile prediction serves as a more effective alternative to pure\nsequence pre-training [ref_3]\n- Furthermore, considering the multi-species and\nmulti-profile nature of genomic profile prediction, we introduce our\n$\\textbf{S}$pecies-$\\textbf{P}$rofile $\\textbf{A}$daptive\n$\\textbf{C}$ollaborative $\\textbf{E}$xperts (SPACE) that leverages Mixture of\nExperts (MoE) to better capture the relationships between DNA sequences across\ndifferent species and genomic profiles, thereby learning more effective DNA\nrepresentations [ref_4]\n- Through extensive experiments across various tasks, our model\nachieves state-of-the-art performance, establishing that DNA models trained\nwith supervised genomic profiles serve as powerful DNA representation learners [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Supervised genomic profile models are trained to predict\nfunctional genomic profiles from DNA sequences (Kathail\net al., 2024). DeepSEA (Zhou & Troyanskaya, 2015) pio-\nneered this paradigm by leveraging convolutional neural net-\nworks (CNNs) to extract DNA sequence features for multi-\ntask prediction. Subsequent works (Kelley et al., 2018; Zhou\net al., 2018; Chen et al., 2022) have continued to advance\nthis direction through either more advanced architectures\nor larger-scale training data. Enformer (Avsec et al., 2021),\nwidely recognized as the SOTA method, achieved superior\nprediction performance through a hybrid Transformer-CNN\narchitecture. While these methods primarily focus on ab\ninitio prediction of genomic profiles from DNA sequences\nand directly utilize these profiles for downstream tasks such\nas variant effect prediction, few studies (Dalla-Torre et al.,\n2024) have explored whether their intermediate represen-\ntations capture meaningful biological patterns. Moreover,\nthese models, which typically adopt a shared encoder cou-\npled with independent profile prediction heads, have not\nthoroughly explored more effective architectural designs\nthat could potentially enhance both prediction performance\nand representation learning.\nUnsupervised DNA foundation models draw from the suc-\ncess of unsupervised pre-training in NLP. DNABERT (Ji\net al., 2021) pioneered this approach, maintaining nearly\nidentical training methods to BERT (Devlin et al., 2019)\nwhile adapting the tokenization scheme to 6-mers (Ce-\nlikkanat et al., 2024) for DNA sequences. Subsequent\nworks have continued along this direction, employing either\nMLM (Zhou et al., 2024; Dalla-Torre et al., 2024; Sanabria\net al., 2024) or NTP (Nguyen et al., 2024a;b) as unsuper-\nvised training objectives. Although these methods have\nmade effective optimizations in terms of training data, model\narchitectures, and tokenization strategies, they still adhere\nto the assumption that unsupervised pre-training on pure\nDNA sequences alone is sufficient for learning effective\nrepresentations. Moreover, there has been little system-\natic comparison between these models and genomic profile\nprediction models in terms of their representation learning\ncapabilities.\nThe MoE framework is a conditional computation tech-\n2\nSPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model\nInputs DNA Sequence\n(Alternating batches from )\nSpecies\nEmbeddingSequence\nRepresentationSpecies\nembeddingStem\nConv T ower\nMHABlock\nCross-Species MoE\nRepresentation of\nsequenceRepresentation of\nsequenceExpert Expert Expert Expert Hidden state of\nsequence\n-Specific\nGating Network\nCross-Species Shared Expert PoolHidden state of\nsequence\n-Specific\nGating Network\nEnhanced profiles\nprediction of Enhanced profiles\nprediction of Profile-Grouped\nEnhancement\nDecoderExpert Species GateLinear\nProjection\nProfile-Specific Expert-Selecte Groups\nSpecies\nEmbedding\nSequence GateSequence\nRepresentationBase Prediction\nExpert Expert Expert Cross-Profile Shared Expert Pool\nEnhanced Prediction\nFinal PredictionConcatenation\nAdd MultiplicationEnformer\nModulesRefinement\nModulesModule\nDetails Categorization\nFigure"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation\n  with Lightweight Specialized LLM\n\n[NUGGETS]\n- Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification [ref_1]\n- While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs [ref_2]\n- Additionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation [ref_3]\n- To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs [ref_4]\n- Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Claim Verification Approaches. Claim verifica-\ntion systems utilize knowledge bases that can be\ncategorized into unstructured and structured for-\nmats. In the unstructured domain, text-based ver-\nification methods predominate, with systems de-\nsigned to verify claims against textual evidence, as\ndemonstrated in the FEVER dataset (Thorne et al.,\n2018). Recent advances have focused on handling\nspecialized verification scenarios, including am-\nbiguous question-answer pairs (Park et al., 2022),\ndetecting factual changes (Schuster et al., 2021),\nand processing multiple documents concurrently\n(Jiang et al., 2020). For structured verification, re-\nsearch has primarily focused on tables and graphs,\nwith early work developing specialized architec-\ntures: graph neural networks for knowledge graph\nprocessing (Zhou et al., 2020), table-specific trans-\nformers (Herzig et al., 2020), and tree-structured\ndecoders for hierarchical data (Wang et al., 2020).\nClaim Verification over Knowledge Graphs\n(KGs). The emergence of Large Language Models\n(LLMs) has simplified direct reasoning over textual\ncorpora for claim verification, as demonstrated by\nProgramFC (Pan et al., 2023) and FOLK (Wang\nand Shu, 2023). However, structured data sources\nlike tables and graphs can provide more grounded\nand robust verification results (Kim et al., 2023b).\nKnowledge graphs are particularly advantageous\nas they enable explicit representation of reason-\ning processes through logical rules over nodes and\nedges. FactKG (Kim et al., 2023b) established a\nfoundation in this direction by introducing a com-\nprehensive dataset for evaluating modern verifica-\ntion methods. KG-GPT (Kim et al., 2023a) fol-\nlowed this work by demonstrating performance\ngains through a pipeline that performs sentence\ndecomposition, subgraph retrieval, and logical in-\nference. Additionally, while not directly addressing\nclaim verification, StructGPT (Jiang et al., 2023)\nand RoG (Luo et al., 2024) achieved promising re-\nsults in related tasks (e.g., Knowledge Base Ques-\ntion Answering) by collecting relevant evidence,\nsuch as subgraphs in KGs, then leveraging LLMs\nfor complex reasoning in particular scenarios."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nUnderstanding Accuracy-Fairness Trade-offs in Re-ranking through\n  Elasticity in Economics\n\n[NUGGETS]\n- Fairness is an increasingly important factor in re-ranking tasks [ref_1]\n- Prior work\nhas identified a trade-off between ranking accuracy and item fairness [ref_2]\n- However,\nthe underlying mechanisms are still not fully understood [ref_3]\n- An analogy can be\ndrawn between re-ranking and the dynamics of economic transactions [ref_4]\n- The\naccuracy-fairness trade-off parallels the coupling of the commodity tax\ntransfer process [ref_5]\n\n[REFERENCES]\n[11] focuses on the Gini Index, other\nwork\n[28] . TaxRank\n", "output": "Fair re-ranking. Over the past decade, work on fair ranking tasks\nhas rapidly grown in volume, driven by the need for a responsi-\nble and trustworthy ecosystem [ 9,21,23,39]. Previous research\noften categorizes fair-aware methods into three categories based on\nranking phases: pre-processing [ 7,37], in-processing [ 34], and post-\nprocessing ( i.e.,re-ranking tasks) [ 28,38]. The re-ranking phase\nis regarded as the most easily adaptable and practical stage in op-\ntimizing ranking systems [ 28]. During the re-ranking phase, the\nconcept of fairness in re-ranking depends on the stakeholders in-\nvolved [ 1,2]. Prior work has examined user-oriented fairness [ 3,19]\nand item-oriented fairness [ 15,28,33,38,41,43]. In this paper, we\nfocus on item group fairness in re-ranking tasks.\nMetrics and algorithms in fair re-ranking. Fairness metrics\nvary widely across works, with different studies optimizing distinct\nmetrics. For instance, some work [ 28,43] employs proportional\nfairness, Do and Usunier [11] focuses on the Gini Index, other\nwork [ 38] prioritizes MMF, and TaxRank [ 41] optimizes\ud835\udefc-fairness.\nHowever, these approaches rely on single fairness metrics, which\nlimits their ability to provide a comprehensive evaluation.\nPrevious work on re-ranking methods to improve item fairness\ncan be divided into (i) regularized methods, which use a multi\u2013\ntask optimization approach with a linear combination of accuracy\nand fairness loss functions, incorporating a trade-off coefficient\n\ud835\udf06[11,26,38], and (ii) constraint-based methods, which formulate\nthe task as a constrained optimization problem to ensure that fair-\nness metrics do not exceed a specified threshold [ 5,28,36,46].\nUnderstanding Accuracy-Fairness Trade-offs in Re-ranking through Elasticity in Economics SIGIR \u201925, July 13\u201318, 2025, Padua, Italy\nTable 1: Correspondence between taxation elements in eco-\nnomics and fair re-ranking.\nEconomics Fair re-ranking\nConsumer (buy product) Users U(click items)\nSupplier (sell product) Item groups G(provide items)\nCommodity tax Fairness constraint\nTax subsidies for the poor Increase ranking score for the poor\nSelling price (tax objective) Ranking scores (fairness objective)\nElasticity on price \ud835\udc38\ud835\udc52 Elasticity on utilities of item group \ud835\udc38\ud835\udc5f,\ud835\udc5d\nDespite achieving notable performance improvements, existing\nfairness intervention methods are often designed to optimize spe-\ncific fairness metrics and typically involve high computational costs,\nmaking them challenging to adapt to real-world industrial systems.\nAn economic perspective on fair re-ranking. In economics, re-\nsource allocation typically occurs through processes of distribution\nand re-distribution [ 17]. Previous work [ 32] regards fair ranking as\na resource allocation problem and formulated the problem related\nto Nash Social Welfare in economics, see also Biswas et al . [5], Patro\net al. [28] . TaxRank [ 41] regards fair re-ranking as a taxation pro-\ncess, which often serves as a key mechanism in the re-distribution\nprocess, enabling wealth reallocation and addressing income in-\nequality [ 13,27]. However, they merely use economic objectives\nto define different fairness metrics, without understanding how\nfairness-accuracy trade-offs occur under different metrics."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nIs Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for\n  LLMs\n\n[NUGGETS]\n- The recent rapid adoption of large language models (LLMs) highlights the\ncritical need for benchmarking their fairness [ref_1]\n- Conventional fairness metrics,\nwhich focus on discrete accuracy-based evaluations (i [ref_2]\n- , prediction\ncorrectness), fail to capture the implicit impact of model uncertainty (e [ref_4]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n", "output": "2.1 Fairness Evaluation of LLMs\nLLM evaluation has become a critical research area (Liang\net al., 2022; Hendrycks et al., 2021; Fourrier et al., 2024),\nespecially the safety of LLMs (Blodgett et al., 2020). While\nefforts have been made in fields such as adversarial robust-\nness (Yang et al., 2024b), toxicity (Hartvigsen et al., 2022),\nand harmfulness (Magooda et al., 2023), gender fairness in\nLLMs remains an important area of research that requires\nfurther exploration and development (Li et al., 2023; Mack-\nraz et al., 2024; Patel et al., 2024).\nCurrently, most model fairness evaluations focus on\nprediction-based metrics (i.e., whether the model predic-\ntions are correct and/or unbiased) (Laskar et al., 2023; Chu\net al., 2024). For example, demographic parity quantifies the\ndifference in positive prediction rates across demographic\ngroups, while equalized odds measures the difference in\nerror.\nOther metrics propose first collecting generated responses\nfrom various tasks (e.g., continuation of input text) and an-\nalyzing the generation quality (e.g., presence of bias) as\nindirect reflections of fairness (Wang et al., 2024). Nonethe-\nless, existing fairness metrics do not consider model uncer-\ntainty, which contains information about a model\u2019s internal\ndecision-making (Ye et al., 2024) and can influence fairness\nestimation.\nBesides metrics, fairness evaluation datasets are another\ncornerstone for measuring LLM fairness (Fabris et al., 2022).\nHowever, existing datasets to assess gender-occupation bias\nin LLMs have limitations. Datasets based on the WinoGrad\nSchema (Levesque et al., 2012), such as WinoBias (Zhao\net al., 2018), WinoBias+ (Vanmassenhove et al., 2021) and\nWinoGender (Rudinger et al., 2018) are no longer adequate\nto evaluate recent LLMs as detailed in Sec"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nKairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded\n  Devices\n\n[NUGGETS]\n- In the era of intelligent manufacturing, anomaly detection has become\nessential for maintaining quality control on modern production lines [ref_1]\n- However,\nwhile many existing models show promising performance, they are often too\nlarge, computationally demanding, and impractical to deploy on\nresource-constrained embedded devices that can be easily installed on the\nproduction lines of Small and Medium Enterprises (SMEs) [ref_2]\n- To bridge this gap, we\npresent KairosAD, a novel supervised approach that uses the power of the Mobile\nSegment Anything Model (MobileSAM) for image-based anomaly detection [ref_3]\n- KairosAD\nhas been evaluated on the two well-known industrial anomaly detection datasets,\ni [ref_4]\n\n[REFERENCES]\n[1] are the architectures most widely used to reconstruct normal\nimages from anomalous ones. Nonetheless, these methods face certain limita-\ntions, especially when reconstructing complex industrial textur\n[22] (Section 2.1). Since KairosAD is based on MobileSAM and em-\nploys efficiency-driven methods to enable deployment on embedded systems,\nthis section also delves into the literature on foundation models \n[25] extends these ideas by incorporating patch-level reconstruction tech-\nniques and scaling to larger architectures and datasets.\nBeyond these, SAM and its efficient variant, such as MobileSAM\n[27] and Segment Anything Model (SAM)\n[29] is a foundational approach that extracts patch-level\nfeature embeddings of normal images into a memory bank, detecting anomalous\npatches during inference through a patch query process. SoftPatch\n", "output": "Over the years, anomaly detection in the industry domain has been extensively\nexplored [22] (Section 2.1). Since KairosAD is based on MobileSAM and em-\nploys efficiency-driven methods to enable deployment on embedded systems,\nthis section also delves into the literature on foundation models for vision (Sec-\ntion 2.2) and efficient deep learning techniques (Section 2.3).\n2.1 Industrial Anomaly Detection\nAlthough extensive research has been carried out on anomaly detection, indus-\ntrial image data presents unique challenges [2].\nMany industrial anomaly detection methods focus on image reconstruction\nand detect anomalies based on the reconstruction error [22]. Generative models\nsuch as autoencoders [3], variational autoencoders [23], and generative adver-\nsarial networks [1] are the architectures most widely used to reconstruct normal\nimages from anomalous ones. Nonetheless, these methods face certain limita-\ntions, especially when reconstructing complex industrial textures and patterns.\nRecentapproachesleveragememorybanks,whereacoresetofstoredfeatures\nfromapre-trainedbackboneisusedtocomputepatch-leveldistancesforanomaly\ndetection [8]. PatchCore [29] is a foundational approach that extracts patch-level\nfeature embeddings of normal images into a memory bank, detecting anomalous\npatches during inference through a patch query process. SoftPatch [14], based\non PatchCore, introduced a patch-level filtering strategy in which patch features\nare filtered and weighted before being stored in the memory bank to reduce con-\ntamination from anomalous patches, thus enhancing model robustness. However,\nmost of these methods come at the cost of increased computational complexity\nand a large memory space, making them unsuitable for deployment on resource-\nconstrained devices.\n1Results calculated by using: https://mlco2.github.io/codecarbon/ .\n4 U. Khan et al.\nIn terms of efficient industrial anomaly detection models, our main competi-\ntor is STLM [19]. Unlike STLM, which employs a two-branch architecture, we\nadopt just a single-branch design, reducing complexity and improving inference\nspeed. Furthermore, while STLM focuses on both anomaly detection and local-\nization, our method is specifically optimized for just the image-level task, prior-\nitizing efficiency for real-time deployment. From a methodological perspective,\nSTLM is based on feature distillation and contrastive learning, while KairosAD\nuses MobileSAM for a lightweight yet effective feature extraction, resulting in a\nsimpler and more computationally efficient model.\n2.2 Foundation Models for Vision\nMultimodal foundation models have emerged as powerful tools across various\ntasks [20]. For visual anomaly detection, particularly relevant are multimodal\napproaches based on CLIP [27] and Segment Anything Model (SAM) [18], as\nwell as vision-only models like DINO [7].\nSpecifically, CLIP learns visual concepts from natural language descriptions\nby training on image-text pairs using a contrastive learning objective that aligns\nembeddingsfrombothmodalities.Thissharedfeaturespaceenablesdownstream\napplications, such as zero-shot image classification, by comparing image embed-\ndings to class-specific textual prompts.\nInstead, DINO adopts a self-supervised student-teacher framework based on\nVision Transformers (ViT), leveraging a multiview strategy to predict softened\nteacher output, resulting in robust and high-quality feature representations. DI-\nNOv2 [25] extends these ideas by incorporating patch-level reconstruction tech-\nniques and scaling to larger architectures and datasets.\nBeyond these, SAM and its efficient variant, such as MobileSAM [37], have\nintroduced a new paradigm in vision models, particularly for segmentation tasks.\nSAM is designed as a powerful promptable segmentation model that generalizes\nwell across diverse images, enabling accurate object delineation with minimal\nsupervision. MobileSAM builds upon SAM, optimizing it for edge devices by\nreducing computational demands while ma"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nFashionDPO:Fine-tune Fashion Outfit Generation Model using Direct\n  Preference Optimization\n\n[NUGGETS]\n- Personalized outfit generation aims to construct a set of compatible and\npersonalized fashion items as an outfit [ref_1]\n- Recently, generative AI models have\nreceived widespread attention, as they can generate fashion items for users to\ncomplete an incomplete outfit or create a complete outfit [ref_2]\n- However, they have\nlimitations in terms of lacking diversity and relying on the supervised\nlearning paradigm [ref_3]\n- Recognizing this gap, we propose a novel framework\nFashionDPO, which fine-tunes the fashion outfit generation model using direct\npreference optimization [ref_4]\n- This framework aims to provide a general fine-tuning\napproach to fashion generative models, refining a pre-trained fashion outfit\ngeneration model using automatically generated feedback, without the need to\ndesign a task-specific reward function [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Fashion Outfit Recommendation. In the fashion domain, Outfit\nRecommendation (OR) [ 20,24,50] has gained widespread appli-\ncation. There are two requirements in fashion outfit recommen-\ndation: compatibility and personalization. Furthermore, it is also\na popular task in the domain of computational fashion [ 2]. Early\nworks [ 6,24,50] primarily focused on compatibility, aiming to re-\ntrieve already well-matched outfits for users. Some works [ 1,5]\nattempt to introduce personalization in the recommendation pro-\ncess, combining a set of personalized and compatible items into an\noutfit that aligns with fashion styling principles. Moreover, bundle\nrecommendation, a more generalized recommendation paradigm,\nsubsume personalized fashion outfit recommendation as one of\nits applications. Multiple works [ 7,23,27] have been proposed by\nusing graph learning, contrastive learning, as well as multimodal\nlarge language models. Despite various progress, the above works\nfollow the retrieval paradigm and are constrained by the variety\nand quantity of fashion products in the dataset, making it difficult\nto meet users\u2019 personalized needs, especially in terms of texture and\nother details. However, with the rapid development of generative\nmodels [ 34,44,51], the quality and diversity of image generation\nhave significantly improved, making it possible to directly recom-\nmend generated custom fashion products to users. Recent work [ 43]\nhas introduced the PFITB task, which combines the user\u2019s inter-\naction history with fashion products to generate a personalized\nmatching outfit.\nFashionDPO: Fine-tune Fashion Outfit Generation Model using Direct Preference Optimization SIGIR \u201925, July 13\u201318, 2025, Padua, Italy\nFashion Image Generation. It refers to the task of generating\nfashion-related images using deep learning models. This task is\nwidely applied in the fashion domain, covering areas such as cloth-\ning design, virtual try-on, and personalized recommendation, among\nothers [ 3,35,46]. Previous works, such as CRAFT [ 15], generate\nfeature representations for clothes pairings and retrieve the most\nsuitable individual clothes items from the dataset. In the virtual\ntry-on domain, previous works [ 10,42] based on GANs involve gen-\nerating warped clothes aligned with character, and then generating\nimages of the character wearing the warped clothes. The diffusion\nmodels [ 8] enhance image quality by replacing the generator in\nthe second stage. Current work [ 16] learns the semantic correspon-\ndence between the clothing and the human body within the latent\nspace of the pre-trained diffusion model in an end-to-end manner.\nIn the personalized recommendation domain, HMaVTON [ 48] gen-\nerates diverse and well-matched fashion items to the given person.\nExisting personalized image generation models [ 14,29,41,49] aim\nto generate images aligned with reference styles or elements, yet\nrecommending images consistent with a user\u2019s interaction history\nis meaningless.\nDirect Preference Optimization. In the field of natural language\nprocessing, Direct Preference Optimization (DPO) has been pro-\nposed to reduce training costs [ 32], which uses preferences rather\nthan explicit rewards to fine-tune LLMs. This approach is also ap-\nplied to the post-training of text-to-image diffusion models. Diffusion-\nDPO [ 40] fine-tunes the generative model in a single step after\nreceiving feedback from the Preference Evaluator. D3PO [ 45] as-\nsumes that the preferred outcome holds true for all time steps in\nthe diffusion model and fine-tunes each of the time steps in the\ngenerative model based on the feedback results. It demonstrates\nthat in diffusion models, directly updating the policy based on hu-\nman preferences within an MDP is equivalent to first learning the\noptimal reward model and then using it to guide policy updates.\nSPO [ 21] assesses preferences at each time step during the sampling\nprocess and adds noise to the preferred image to generate the noise\nimage for the next time ste"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nMeasure Domain's Gap: A Similar Domain Selection Principle for\n  Multi-Domain Recommendation\n\n[NUGGETS]\n- Multi-Domain Recommendation (MDR) achieves the desirable recommendation\nperformance by effectively utilizing the transfer information across different\ndomains [ref_1]\n- Despite the great success, most existing MDR methods adopt a single\nstructure to transfer complex domain-shared knowledge [ref_2]\n- However, the beneficial\ntransferring information should vary across different domains [ref_3]\n- When there is\nknowledge conflict between domains or a domain is of poor quality,\nunselectively leveraging information from all domains will lead to a serious\nNegative Transfer Problem (NTP) [ref_4]\n- Therefore, how to effectively model the\ncomplex transfer relationships between domains to avoid NTP is still a\ndirection worth exploring [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "6.1 Multi-Domain Recommendation\nRecommendation systems (RS) [ 14,19,20,31,32,34,35,57] aims\nto analyze user interactions to uncover interests, becoming a key\nresearch focus in recent years. However, classical single-domain\napproaches are unable to process the multi-domain data, which are\noften encountered in real-world applications. As a result, abundant\nMulti-Domain Recommendation (MDR) methods have been pro-\nposed [ 5,6,9,10,13,28,37], leveraging shared knowledge across\ndomains to address challenges such as cold-start issues [ 21,56,72].\nThese methods can be broadly categorized into Shared-Specific\n(SS) based methods and Dynamic Weight (DW) based methods, de-\npending on how they model inter-domain relationships. SS-based\nmethods[ 41,50,51], such as STAR [ 45] employ a shared-bottom\narchitecture with domain-specific towers to model features. WhileDW-based methods [ 3,61,65] often use scenario-sensitive features\nto generate weighted parameters for the network.\nHowever, DW-based methods rely on manually selected features\nand hence are less generalizable when new scenarios are encoun-\ntered. Furthermore, most SS-based approaches [ 58,59] employ a sin-\ngle domain-shared module, making it difficult to transfer complex\nmulti-domain knowledge. To tackle these issues, SDSP proposes a\nnovel domain selection module that can decouple the current single\ndomain-shared without additional feature engineering.\n6.2 Selection Problem\nDiscrete selection problems are generally more challenging than\ncontinuous optimization problems. This is because discrete choices\ninvolve combinatorial complexity, where the solution space is not\nsmooth or continuous. Thus, traditional optimization techniques\nlike gradient-based methods cannot be directly applied, requiring\nspecialized algorithms to explore the solution space efficiently.\nIn some fields, several attempts [ 70,71] have been proposed to\nsolve different selection problems. Standley et al. [ 47] propose a\ngroup framework for choosing the suitable tasks to train together in\nthe multi-task field. In the multi-modal field, He et al. [ 18] proposes\na greedy modality selection algorithm via submodular maximiza-\ntion. In the cross-domain field, Park et al. [ 42] devise a weight factor\nto control the negative transfer of the multi-domain part.\nHowever, the greedy-based search algorithm incurs additional\noverhead and is not applicable in the time-sensitive field. Besides,\na single gating mechanism doesn\u2019t apply to the complex multi-\ndomain field. To address these issues, SDSP proposes a dynamic\nselection method to tackle the selection problem efficiently."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nProximal Algorithm Unrolling: Flexible and Efficient Reconstruction\n  Networks for Single-Pixel Imaging\n\n[NUGGETS]\n- Deep-unrolling and plug-and-play (PnP) approaches have become the de-facto\nstandard solvers for single-pixel imaging (SPI) inverse problem [ref_1]\n- PnP\napproaches, a class of iterative algorithms where regularization is implicitly\nperformed by an off-the-shelf deep denoiser, are flexible for varying\ncompression ratios (CRs) but are limited in reconstruction accuracy and speed [ref_2]\n- Conversely, unrolling approaches, a class of multi-stage neural networks where\na truncated iterative optimization process is transformed into an end-to-end\ntrainable network, typically achieve better accuracy with faster inference but\nrequire fine-tuning or even retraining when CR changes [ref_3]\n- In this paper, we\naddress the challenge of integrating the strengths of both classes of solvers [ref_4]\n- To this end, we design an efficient deep image restorer (DIR) for the unrolling\nof HQS (half quadratic splitting) and ADMM (alternating direction method of\nmultipliers) [ref_5]\n\n[REFERENCES]\n[36] shows that under homogeneity, nonexpansiveness and Jaco-\nbian symmetry conditions, a denoiser can be written as a\ngradient descent step on a convex function. However, such\nconditions are unrealistic f\n", "output": "SPI Reconstruction. SPI reconstruction is a classical in-\nverse problem in the field of compressive imaging [48\u2013\n50, 59]. Mainstream solvers involve iterative optimization\nalgorithms [3, 5, 10, 16, 21, 26, 28, 30, 53], PnP algo-\nrithms [15, 22\u201324, 66], single-stage neural networks [27,\n39, 40, 55], and unrolling (multi-stage) neural networks [18,\n29, 32, 35, 38, 41, 42, 47, 51, 54, 56, 57, 61, 62, 68]. Itera-\ntive algorithms employ hand-crafted regularizers, e.g., spar-\nsity [16], total variation [28], non-local low rank [10, 58],\nwith a proximal algorithm, e.g., iterative shrinkage thresh-\nolding algorithm (ISTA) [3], approximate message passing\n(AMP) [30], HQS [17], and ADMM [53]. Single-stage net-\nworks generally achieve inferior performance due to the in-\nsufficient utilization of imaging model information. PnP\nalgorithms are flexible for varying CRs and unrolling net-\nworks achieve superior performance, thus making both the\nde-facto standard tools for SPI reconstruction.\nProximal learning. The objective of proximal learning is to\ntrain a neural network as the proximal operator of a explicit\nregularization function. Regularization by denoising [36]\nshows that under homogeneity, nonexpansiveness and Jaco-\nbian symmetry conditions, a denoiser can be written as a\ngradient descent step on a convex function. However, such\nconditions are unrealistic for deep denoisers. Recently, a\nnew type of gradient denoisers [15, 23, 24] has been pro-\nposed by training a denoiser as an explicit gradient step\non a functional parameterized by a deep neural network.\nHowever, these denoisers must either be a contractive gra-\ndient [23, 24] or be constrained to input convex neural net-\nworks (ICNN) [15], inevitably sacrificing the expressivity.\nProximal learning without assumptions and constraints re-\nmains an open challenge."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nRefining Datapath for Microscaling ViTs\n\n[NUGGETS]\n- Vision Transformers (ViTs) leverage the transformer architecture to\neffectively capture global context, demonstrating strong performance in\ncomputer vision tasks [ref_1]\n- A major challenge in ViT hardware acceleration is that\nthe model family contains complex arithmetic operations that are sensitive to\nmodel accuracy, such as the Softmax and LayerNorm operations, which cannot be\nmapped onto efficient hardware with low precision [ref_2]\n- Existing methods only\nexploit parallelism in the matrix multiplication operations of the model on\nhardware and keep these complex operations on the CPU [ref_3]\n- This results in\nsuboptimal performance due to the communication overhead between the CPU and\naccelerator [ref_4]\n- Can new data formats solve this problem [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "I. I NTRODUCTION\nHardware acceleration for transformers has shown sig-\nnificant performance benefits compared to general proces-\nsors [1], [2], [3], among which Vision Transformers (ViTs)\noffer promising performance for capturing global image re-\nlationships [4]. Compared to traditional Convolutional Neural\nNetworks (CNNs), ViTs present new model features: 1) these\nmodels often contain millions of parameters, leading to a\nlarge memory size; and 2) they contain non-linear operations,\nrequiring complex hardware operator designs.\nTraditional techniques for ViT acceleration focus on 1)\ninteger quantization and2) datapath optimization , exploiting\nthe approximation tolerance of ViT models. First, integer\nquantization represents numbers as small integers, optionally\nwith a scaling factor, leading to both smaller memory and cir-\ncuit area [2], [3]. Second, the datapath optimization determines\nnew designs with simpler logic and similar results, leading to\na smaller circuit area [5].\nStill, the non-linear operations in ViT, such as LayerNorm\nand Softmax, face challenges in efficient acceleration. These\noperations contain complex mathematical operations, such as\nexp() and sqrt() , and require large value ranges, restricting\nexisting integer quantization. Existing design methods rely on\nthe CPU and only accelerate part of the ViT models in FPGA\nfabric [2], [3]. This leads to a working but complex systemTABLE I: Our MXInt design method maps all non-linear\noperations in ViTs into efficient hardware, achieving lower\nbitwidths than traditional fixed-point designs."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nUnlearning for Federated Online Learning to Rank: A Reproducibility\n  Study\n\n[NUGGETS]\n- This paper reports on findings from a comparative study on the effectiveness\nand efficiency of federated unlearning strategies within Federated Online\nLearning to Rank (FOLTR), with specific attention to systematically analysing\nthe unlearning capabilities of methods in a verifiable manner [ref_1]\n- Federated approaches to ranking of search results have recently garnered\nattention to address users privacy concerns [ref_2]\n- In FOLTR, privacy is safeguarded\nby collaboratively training ranking models across decentralized data sources,\npreserving individual user data while optimizing search results based on\nimplicit feedback, such as clicks [ref_3]\n- Recent legislation introduced across numerous countries is establishing the\nso called \"the right to be forgotten\", according to which services based on\nmachine learning models like those in FOLTR should provide capabilities that\nallow users to remove their own data from those used to train models [ref_4]\n- This has\nsparked the development of unlearning methods, along with evaluation practices\nto measure whether unlearning of a user data successfully occurred [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "2.1 Federated Online Learning to Rank\nAs a specialized distributed machine learning paradigm, FL enables\ncollaborative training while preserving user privacy. To satisfy\nprivacy-preserving requirements in OLTR, FL has also been intro-\nduced to this field termed federated online learning to rank (FOLTR).\nThe FOLtR-ES method does this by incorporating evolution strate-\ngies to optimize the ranking model and \ud835\udf16-local differential privacy\nfor enhanced data protection [ 14], but this methods is found to have\nreduced effectiveness and adaptability on large-scale datasets [ 28].\nA more effective approach, FPDGD [ 26], has been proposed by in-\ntegrating the advanced Pairwise Differentiable Gradient Descent\n(PDGD) [ 19] into the FL framework. Thanks to the superiority of\nlocally-deployed PDGD ranker in handling noise and biases from\nuser interactions, FPDGD has demonstrated strong ranking perfor-\nmance which is comparable to that of the state-of-the-art centralised\nOLTR methods. Following this, Wang and Zuccon [ 29] investigate\nthe robustness of FPDGD to non-independent and identically dis-\ntributed client data. The vulnerability of FPDGD to poisoning at-\ntacks in the federated system is also simulated and verified [ 30],\nwhich extends the landscape of study on FOLTR.\n2.2 Federated Unlearning\nFederated Unlearning extends the principles of machine unlearning\nto FL settings, allowing individual clients\u2019 data to be removed from\na global model. A naive approach in FU is to retrain the global\nmodel from scratch without involving clients who requested dele-\ntion, ensuing complete removal but at a high computational cost.\nIn order to accelerate the FU process, FedEraser adjusts historical\nupdates from clients to reconstruct unlearned models [ 15]; other\ncomputation-efficient approaches have followed [ 11,16,31]. While\nFU has been studied across natural language processing and recom-\nmendation systems [ 33,37], its application to FOLTR tasks remains\nunexplored, with Wang et al. [ 27] presenting the only approach to\ndate. This study implemented an existing FU approach and evalu-\nated its unlearning performance under a user simulation inspired\nby poisoning attacks [ 24,30]. While Wang et al. [ 27] pointed out\nthat the uniqueness of FOLTR including optimising ranking tasks\nwith implicit user feedback in an online manner, the challenge of\nUnlearning for Federated Online Learning to Rank: A Reproducibility Study SIGIR \u201925, July 13\u201318, 2025, Padua, Italy\nadapting existing FU methods to FOLTR and evaluating the unlearn-\ning performance are largely unexplored. In this paper, we close this\ngap by evaluating within the context of FOLTR the performance\nof a broad range of unlearning methods across a diverse set of\nevaluation metrics."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nAligning Protein Conformation Ensemble Generation with Physical Feedback\n\n[NUGGETS]\n- Protein dynamics play a crucial role in protein biological functions and\nproperties, and their traditional study typically relies on time-consuming\nmolecular dynamics (MD) simulations conducted in silico [ref_1]\n- Recent advances in\ngenerative modeling, particularly denoising diffusion models, have enabled\nefficient accurate protein structure prediction and conformation sampling by\nlearning distributions over crystallographic structures [ref_2]\n- However, effectively\nintegrating physical supervision into these data-driven approaches remains\nchallenging, as standard energy-based objectives often lead to intractable\noptimization [ref_3]\n- In this paper, we introduce Energy-based Alignment (EBA), a\nmethod that aligns generative models with feedback from physical models,\nefficiently calibrating them to appropriately balance conformational states\nbased on their energy differences [ref_4]\n- Experimental results on the MD ensemble\nbenchmark demonstrate that EBA achieves state-of-the-art performance in\ngenerating high-quality protein ensembles [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Protein conformation generation. Unlike structure pre-\ndiction (Jumper et al., 2021) aiming to identify a single,\nmost-likely folded structure, protein conformation genera-\ntion focuses on sampling an ensemble of physically plau-\nsible states that capture the underlying energy landscape.\nBoltzmann generator (No \u00b4e et al., 2019) leverages normaliz-\ning flows to approximate the Boltzmann distribution by train-\ning on simulation data. Arts et al. (2023) applies the diffu-\nsion model to capture such distribution over coarse-grained\nprotein conformations. EigenFold (Jing et al., 2023) adopts\na generative perspective on structure prediction, enabling the\ngeneration of multiple structures given an input sequence.\nStr2Str (Lu et al., 2024b) introduces a score-based sampler\ntrained exclusively on PDB data, framing conformation gen-\neration in a structure-to-structure paradigm. DiG (Zheng\net al., 2024) trains a conditional diffusion model on both\nPDB and in-house simulation data. ConfDiff (Wang et al.,\n2024) incorporates the energy- and force-guidance during\nthe reverse process of diffusion to enhance the accuracy of\nconformation generation. AlphaFlow (Jing et al., 2024a)\nrepurposes the AlphaFold2 model into a denoising network\nvia flow matching. ESMDiff (Lu et al., 2024a) fine-tunes\nthe protein language model ESM3 using discrete diffusion\nto produce protein conformations. Finally, MDGen (Jing\net al., 2024b) attempts direct generation of MD trajectories\nby modeling them as time-series of protein structures.\nAlignment methods for generative models. Aligning\ngenerative models with desired objectives is becoming in-\ncreasingly important. The Reinforcement Learning from\nHuman Feedback (RLHF) framework optimizes models via\nRL using human preference rewards and has been widely ap-\nplied in tasks like machine translation (Kreutzer et al., 2018),\nsummarization (Stiennon et al., 2020), and instruction fol-\nlowing (Ouyang et al., 2022). RLHF has also been applied\nfor alignment of text-to-image diffusion models (Black et al.,\n2023; Fan et al., 2024). However, RL-based fine-tuning\nfaces significant challenges in stability and scalability. Di-\nrect Preference Optimization (Rafailov et al., 2024) mit-\nFigure 3. (Top) Structure ensembles for the target 6uof Ain AT-\nLAS test set with RMSF correlation rlabeled. (Bottom) C \u03b1-\nRMSF versus the residue index (N \u2192C terminus from left to\nright).\nigates these issues by directly optimizing for the optimal\npolicy via re-parameterization of an implicit reward model.\nThis approach has been extended beyond language model-\ning: Diffusion-DPO (Wallace et al., 2024) for text-to-image\ngeneration, ABDPO (Zhou et al., 2024) for antibody design\nusing Rosetta energy (Alford et al., 2017), and ALIDIFF\n(Gu et al., 2024) and DECOMPDPO (Cheng et al., 2024)\nfor molecular optimization in structure-based drug design.\nRemarks: Our method differs from existing approaches\nabove by adopting a more general-form objective, being\ngrounded in physically meaningful motivations, addressing\na different task and demonstrating superior performance."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nPre-Training Curriculum for Multi-Token Prediction in Language Models\n\n[NUGGETS]\n- Multi-token prediction (MTP) is a recently proposed pre-training objective\nfor language models [ref_1]\n- Rather than predicting only the next token (NTP), MTP\npredicts the next $k$ tokens at each prediction step, using multiple prediction\nheads [ref_2]\n- MTP has shown promise in improving downstream performance, inference\nspeed, and training efficiency, particularly for large models [ref_3]\n- However, prior\nwork has shown that smaller language models (SLMs) struggle with the MTP\nobjective [ref_4]\n- To address this, we propose a curriculum learning strategy for MTP\ntraining, exploring two variants: a forward curriculum, which gradually\nincreases the complexity of the pre-training objective from NTP to MTP, and a\nreverse curriculum, which does the opposite [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Curriculum learning. After Bengio et al. (2009)\nfirst proposed to apply a curriculum learning strat-egy in the context of machine learning, it has been\nsuccessfully applied on a number of tasks in var-\nious machine learning domains including natural\nlanguage processing, computer vision and speech\nrecognition (Soviany et al., 2022). In the context of\nlanguage modeling, they have been shown to pro-\nvide benefits both when pre-training encoder-only\nmodels (Xu et al., 2020; Nagatsuka et al., 2021;\nRanaldi et al., 2023), as well as instruction-tuning\nlarge decoder-only models (Mukherjee et al., 2023;\nLee et al., 2024).\nThe use of curriculum learning approaches was\nnot reported in pre-training any publicly available\ndecoder-only foundation models trained on vast\namounts of text data, although recently Feng et al.\n(2024) showed that using a two-stage curriculum\nbased on text quality can lead better training out-\ncomes. Meanwhile curriculum learning approaches\nhave been very popular in data-constrained pre-\ntraining setups (Warstadt et al., 2023; Hu et al.,\n2024). While the curricula that focus on order-\ning the data based on various difficulty metrics\nwere not found to be consistently better than non-\ncurriculum baselines, an approach by Salhan et al.\n(2024) that involves a curriculum for pre-training\nobjectives was able to reliably outperform non-\ncurriculum baselines in a data-constrained setup.\nMulti-token prediction. ProphetNet (Qi et al.,\n2020) was the first large-scale transformer-based\nlanguage model that was able to predict multiple n-\ngrams in one prediction step. However, their model\nrelies on n-stream self-attention mechanism that\ninvolves more computational overhead compared\nto regular transformers.\nPal et al. (2023) showed that hidden states of\nnext-token prediction models are able to encode\nmore than a single token ahead by probing pre-\ntrained transformers, and that it\u2019s possible to pre-\ndict those to a certain extent.\nGloeckle et al. (2024) improved upon the previ-\nous work by proposing slight architectural tweaks,\nsuch as using full transformer layers as language\nmodeling heads, to account for the multi-token pre-\ndiction task that resulted in a more computationally\nefficient, compute-matched with NTP models, and\neffective method for multi-token prediction.\nSelf-speculative decoding. Stern et al. (2018)\nwere the first to suggest a speculative decoding\nscheme for faster inference. Since then, a number\nself-speculative decoding methods were introduced.\nSome of these methods rely on the early-exit mech-\nanism (Elhoushi et al., 2024; Liu et al., 2024b), oth-\n8\ners on skipping intermediate layers (Zhang et al.,\n2024a; Xia et al., 2024), and some on architectural\ntransformations (Zhang et al., 2024b). Medusa (Cai\net al., 2024) has gained the most prominence due\nto its simplicity and ability to relatively easily and\ncost-efficiently enable self-speculative decoding\nfor LLMs that were pre-trained using the regular\nNTP objective."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nKubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network\n  Costs\n\n[NUGGETS]\n- Modern cloud-native applications increasingly utilise managed cloud services\nand containerisation technologies, such as Kubernetes, to achieve rapid\ntime-to-market and scalable deployments [ref_1]\n- Organisations must consider various\nfactors, including cost implications when deciding on a hosting platform for\ncontainerised applications as the usage grows [ref_2]\n- An emerging discipline called\nFinOps combines financial management and cloud operations to optimise costs in\ncloud-based applications [ref_3]\n- While prior research has explored system-level\noptimisation strategies for cost and resource efficiency in containerized\nsystems, analysing network costs in Kubernetes clusters remains underexplored [ref_4]\n- This paper investigates the network usage and cost implications of\ncontainerised applications running on Kubernetes clusters [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "Section 3 details the methodology, and Section 4 presents the experimental setup\nand results. Section 5 discusses the findings and their implications, while Section 6\nconcludes the paper.\nKubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network Costs 3"}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nPS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and\n  Optimization\n\n[NUGGETS]\n- Neural rendering methods have gained significant attention for their ability\nto reconstruct 3D scenes from 2D images [ref_1]\n- The core idea is to take multiple\nviews as input and optimize the reconstructed scene by minimizing the\nuncertainty in geometry and appearance across the views [ref_2]\n- However, the\nreconstruction quality is limited by the number of input views [ref_3]\n- This limitation\nis further pronounced in complex and dynamic scenes, where certain angles of\nobjects are never seen [ref_4]\n- In this paper, we propose to use video frame\ninterpolation as the data augmentation method for neural rendering [ref_5]\n\n[REFERENCES]\n[7] addresses\nthe computational challenges of traditional NeRF imple-mentations by introducing fast neural initialization tech-\nniques, allowing for efficient and scalable 3D scene recon-\nstructions. Thes\n[21] introduces an optical-flow network to en-\nhance view alignment during training, thereby improving\nhigh-frequency details in reconstructed scenes. Diffusion-\nNeRF\n[25] incorporates the diffusion process after initial reconstruc-\ntion to refine the quality of novel views. However, while ef-\nfective on low-quality reconstructions, these methods intro-\nduce artifacts t\n[39] pushes\nthis further by optimizing NeRF for autonomous driving,\nfocusing on modeling physical sensors and improving the\nfidelity of object-level details to shorten the simulation to\nreal-world gap. Mea\n[42] employs a diffusion model to learn gradients\nof RGBD patch priors, providing regularized geometry and\ncolor information for each scene. The 3DGS-enhancer\n[45] introduces a unified simulation approach\nthat leverages NeRF-based scene representations to gener-\nate highly realistic synthetic data, significantly aiding the\ntraining of autonomous driving models. \n", "output": "Video Frame Interpolation: Video frame interpolation\n(VFI) technique has been widely studied in recent years\ndue to its significance in various video processing appli-\ncations, including generating smooth slow-motion videos,\nincreasing video frame rates, and enhancing visual qual-\nity. Conventional VFI methods rely on model-based motion\nestimation, blending, and morphing techniques [11, 33],\nwhich can be computationally intensive and prone to arti-\nfacts such as ghosting or blurring. Recent advances in deep\nlearning have significantly improved VFI techniques, ad-\ndressing many limitations of conventional methods. This\nhas led to the emergence of end-to-end neural networks\nfor VFI, such as Depth-Aware Video Frame Interpola-\ntion (DAIN) [2], Real-Time Intermediate Flow Estimation\n(RIFE) [18], Many-to-Many Splatting (M2M) [17], and\nEMA-VFI [48]. These methods explore various approaches\nincluding mixed, sequential, and parallel feature extrac-\ntions, and leverage advanced techniques like deformable\nconvolution and depth estimation to produce more accurate\nand visually appealing interpolated frames. Despite the im-\nprovements, challenges such as handling large motion arti-\nfacts and preserving fine details remain, driving further re-\nsearch in this area.\nAutonomous Driving NeRF: Autonomous driving re-\nsearchers have incorporated neural radiance fields (NeRF)\nto reconstruct 3D scenes and simulate safety-critical scenar-\nios. UniSim [45] introduces a unified simulation approach\nthat leverages NeRF-based scene representations to gener-\nate highly realistic synthetic data, significantly aiding the\ntraining of autonomous driving models. Neurad [39] pushes\nthis further by optimizing NeRF for autonomous driving,\nfocusing on modeling physical sensors and improving the\nfidelity of object-level details to shorten the simulation to\nreal-world gap. Meanwhile, Lightning-NeRF [7] addresses\nthe computational challenges of traditional NeRF imple-mentations by introducing fast neural initialization tech-\nniques, allowing for efficient and scalable 3D scene recon-\nstructions. These methods collectively advance the integra-\ntion of NeRFs in autonomous driving by improving both the\nquality and speed of 3D scene representations.\nNeural Rendering Enhancement: To address chal-\nlenges in neural rendering arising from insufficient or\nlow-quality data, several methods have been developed.\nAlignNeRF [21] introduces an optical-flow network to en-\nhance view alignment during training, thereby improving\nhigh-frequency details in reconstructed scenes. Diffusion-\nNeRF [42] employs a diffusion model to learn gradients\nof RGBD patch priors, providing regularized geometry and\ncolor information for each scene. The 3DGS-enhancer [25]\nincorporates the diffusion process after initial reconstruc-\ntion to refine the quality of novel views. However, while ef-\nfective on low-quality reconstructions, these methods intro-\nduce artifacts to high-quality reconstructions and substan-\ntially extend the training time, further prolonging an already\nlengthy process [47]."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nDon't Reinvent the Wheel: Efficient Instruction-Following Text Embedding\n  based on Guided Space Transformation\n\n[NUGGETS]\n- In this work, we investigate an important task named instruction-following\ntext embedding, which generates dynamic text embeddings that adapt to user\ninstructions, highlighting specific attributes of text [ref_1]\n- Despite recent\nadvancements, existing approaches suffer from significant computational\noverhead, as they require re-encoding the entire corpus for each new\ninstruction [ref_2]\n- To address this challenge, we propose GSTransform, a novel\ninstruction-following text embedding framework based on Guided Space\nTransformation [ref_3]\n- Our key observation is that instruction-relevant information is\ninherently encoded in generic embeddings but remains underutilized [ref_4]\n- Instead of\nrepeatedly encoding the corpus for each instruction, GSTransform is a\nlightweight transformation mechanism that adapts pre-computed embeddings in\nreal time to align with user instructions, guided by a small amount of text\ndata with instruction-focused label annotation [ref_5]\n\n[REFERENCES]\n[1] Reference 1\n[2] Reference 2\n[3] Reference 3\n[4] Reference 4\n[5] Reference 5\n", "output": "2.1 Generic Text Embedding\nText embedding has been a long-studied prob-\nlem. Since word embeddings, people adopt self-\nsupervised training in generating word embed-\ndings, and pool the word embeddings to form\ntext embeddings (Mikolov et al., 2013; Penning-\nton et al., 2014). Recent advancements in context-\naware semantic text embedding models leverage\nTransformer-based architectures (Vaswani et al.,\n2017; Devlin et al., 2019) as their backbone, often\nemploying customized objectives like contrastive\nloss to train the models (Cer et al., 2018; Reimers\nand Gurevych, 2019; Gao et al., 2021; Zhuo et al.,\n2023). Moreover, state-of-the-art (SOTA) text em-\nbedding models have been further enhanced with\ntechniques such as using large language models\n(LLMs) (Wang et al., 2023; Muennighoff et al.,\n2025; Lei et al., 2024) and more sophisticated loss\nfunctions designed to address issues like cosine\nsaturation (Li and Li, 2024).\nDespite their effectiveness, these methods lack\ngeneralizability and fail to meet diverse user needs\nwhen downstream tasks require focusing on spe-\ncific aspects beyond general semantics.\n2.2 Instruction-Following Text Embedding\nInstruction-following text embedding (Su et al.,\n2023; Peng et al., 2024) allows users to guide\nembedding generation through customized instruc-\ntions. The model produces embeddings that align\nwith users\u2019 specific interests by considering both\nthe input text and instructions.\nInstructOR (Su et al., 2023) pioneered\ninstruction-based embeddings by concatenating\ninstructions with input texts and training the model\nusing contrastive objectives across a diverse set\nof instructions. It adapts embeddings for varied\nsemantic interpretations but does not explicitly\nmodel instruction-specific semantic aspects.\nInBedder (Peng et al., 2024) extends this idea by\ntreating instructions as questions and generating\nintermediate answers to produce more fine-grained,\ninstruction-aware embeddings. They also propose\nInstruction Awareness Tests, which we adopt to\nevaluate Triplet Alignment, STS, and Clustering\ntasks. Yet, both methods require re-encoding the\nentire corpus for each new instruction, resulting\nin notable computational overhead and latency,\nespecially for large-scale datasets.\nBeyond text embeddings, related efforts have\nexplored instruction-aware and prompt-based infor-\nmation retrieval (Weller et al., 2025b; Min et al.,\n2025; Oh et al., 2024; Sun et al., 2024; Weller et al.,\n2025a), offering alternative formulations that lever-\nage user intent to enhance retrieval quality."}
{"input": "You are writing a Related Work section for a scientific paper.\nUse the following nuggets (facts) and references. Write well-structured paragraphs and include inline citations like [1], [2], ... matching the reference indices.\n\n[QUERY]\nStitching Inner Product and Euclidean Metrics for Topology-aware Maximum\n  Inner Product Search\n\n[NUGGETS]\n- Maximum Inner Product Search (MIPS) is a fundamental challenge in machine\nlearning and information retrieval, particularly in high-dimensional data\napplications [ref_1]\n- Existing approaches to MIPS either rely solely on Inner Product\n(IP) similarity, which faces issues with local optima and redundant\ncomputations, or reduce the MIPS problem to the Nearest Neighbor Search under\nthe Euclidean metric via space projection, leading to topology destruction and\ninformation loss [ref_2]\n- Despite the divergence of the two paradigms, we argue that\nthere is no inherent binary opposition between IP and Euclidean metrics [ref_3]\n- By\nstitching IP and Euclidean in the design of indexing and search algorithms, we\ncan significantly enhance MIPS performance [ref_4]\n- Specifically, this paper explores\nthe theoretical and empirical connections between these two metrics from the\nMIPS perspective [ref_5]\n\n[REFERENCES]\n[15] quantizes the norms of\nitems in a dataset explicitly to reduce errors in norm. ScaNN\n[22] improves graph\nquality with angular proximity. M\u00f6bius-Graph\n[23] improves\non this but faces difficulties with negative inner product values.\nQuantization-based methods :NEQ\n[26] builds De-\nlaunay graphs via inner product. ip-NSW+\n[27] introduce a symmetric LSH that enjoys\nstrong guarantees. Fargo\n[36] addresses this\nby projecting vectors into a lower-dimensional space, though infor-\nmation loss remains a challenge. LRUS-CoverTree\n[37] employs an orthogonality-amplified residual loss and have\nbecome state-of-the-art and been integrated into ScaNN library.\nGraph-based methods : Proven effective for NNS , graph-based\nmethods have been\n[38] uses a norm-adaptive inner product ( \ud835\udefc\u27e8\ud835\udc65,\ud835\udc66\u27e9) inip-NSW .\n[46] is the first to observe that MIPS results cluster around large-norm\nvectors. Simple- LSH\n[49] adopts M\u00f6bius\ntransforms for MIPS .IPDG prunes extreme points for top-1 MIPS.\nNAPG\n", "output": "Inner Product is crucial in AIand machine learning applications\nsuch as representation learning,language modeling, computer vi-\nsion and recommender systems [ 7,20,30,41,44,47].MIPS methods\nare generally categorized into Locality Sensitive Hashing ( LSH),\ntree-, quantization-, and graph-based approaches:\nLSH-based methods : Traditional LSH [40,43], originally designed\nfor Euclidean space, is adapted for MIPS using transformations\nsuch as\ud835\udc3f2[34], Correlation [ 35], and XBOX [9]. Range- LSH [46]\nis the first to observe that MIPS results cluster around large-norm\nvectors. Simple- LSH [27] introduce a symmetric LSH that enjoys\nstrong guarantees. Fargo [ 48] represents the recent state-of-the-art.\nTree-based methods : Early MIPS approaches favored trees but\nstruggled with high dimensionality. ProMIPS [36] addresses this\nby projecting vectors into a lower-dimensional space, though infor-\nmation loss remains a challenge. LRUS-CoverTree [23] improves\non this but faces difficulties with negative inner product values.\nQuantization-based methods :NEQ [15] quantizes the norms of\nitems in a dataset explicitly to reduce errors in norm. ScaNN [ 19]\nintegrates \"VQ-PQ\" with anisotropic quantization loss, while SOAR\n[37] employs an orthogonality-amplified residual loss and have\nbecome state-of-the-art and been integrated into ScaNN library.\nGraph-based methods : Proven effective for NNS , graph-based\nmethods have been adapted for MIPS .ip-NSW [26] builds De-\nlaunay graphs via inner product. ip-NSW+ [22] improves graph\nquality with angular proximity. M\u00f6bius-Graph [49] adopts M\u00f6bius\ntransforms for MIPS .IPDG prunes extreme points for top-1 MIPS.\nNAPG [38] uses a norm-adaptive inner product ( \ud835\udefc\u27e8\ud835\udc65,\ud835\udc66\u27e9) inip-NSW ."}
